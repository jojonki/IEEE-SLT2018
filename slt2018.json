[
    {
        "Abstract": "Global Style Tokens (GSTs) are a recently-proposed method to learn latent disentangled representations of high-dimensional data. GSTs can be used within Tacotron, a state-of-the-art end-to-end speech synthesis system, to uncover expressive factors of variation in speaking style. In this work, we introduce the Text-Predicting Global Style Token (TP-GST) architecture, which treats GST combination weights or style embeddings as ``virtual'' speaking style labels within Tacotron. TP-GST learns to predict stylistic renderings from text alone, requiring neither explicit labels during training, nor auxiliary inputs for inference. We show that, when trained on an expressive speech dataset, our system can render text with more pitch and energy variation than two state-of-the-art baseline models. We further demonstrate that TP-GSTs can synthesize speech with background noise removed, and corroborate these analyses with positive results on human-rated listener preference audiobook tasks. Finally, we demonstrate that multi-speaker TP-GST models successfully factorize speaker identity and speaking style. We provide a website with audio samples for each of our findings.",
        "Authors": [
            "Daisy Stanton; Google",
            "Yuxuan Wang; Google",
            "RJ Ryan; Google"
        ],
        "Paper Title": "PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "1",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "Generative networks can create an artificial spectrum based on its conditional distribution estimate instead of predicting only the mean value, as the Least Square (LS) solution does. This is promising since the LS predictor is known to oversmooth features leading to muffling effects. However, modeling a whole distribution instead of a single mean value requires more data and thus also more computational resources. With only one hour of recording, as often used with LS approaches, the resulting spectrum is noisy and sounds full of artifacts. In this paper, we suggest a new loss function, by mixing the LS error and the loss of a discriminator trained with Wasserstein GAN, while weighting this mixture differently through the frequency domain. Using listening tests, we show that, using this mixed loss, the generated spectrum is smooth enough to obtain a decent perceived quality. While making our source code available online, we also hope to make generative networks more accessible with lower the necessary resources.",
        "Authors": [
            "Gilles Degottex; ObEN, Inc. - University of Cambridge",
            "Mark Gales; University of Cambridge"
        ],
        "Paper Title": "A SPECTRALLY WEIGHTED MIXTURE OF LEAST SQUARE ERROR AND WASSERSTEIN DISCRIMINATOR LOSS FOR GENERATIVE SPSS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "2",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "Most neural-network based speaker-adaptive acoustic models for speech synthesis can be categorized into either layer-based or input-code approaches. Although both approaches have their own pros and cons, most existing works on speaker adaptation focus on improving one or the other. In this paper, after we first systematically overview the common principles of neural-network based speaker-adaptive models, we show that these approaches can be represented in a unified framework and can be generalized further. More specifically, we introduce the use of scaling and bias codes as generalized means for speaker-adaptive transformation. By utilizing these codes, we can create a more efficient factorized speaker-adaptive model and capture advantages of both approaches while reducing their disadvantages. The experiments show that the proposed method can improve the performance of speaker adaptation compared with speaker adaptation based on the conventional input code.",
        "Authors": [
            "Hieu-Thi Luong; National Institute of Informatics",
            "Junichi Yamagishi; National Institute of Informatics"
        ],
        "Paper Title": "SCALING AND BIAS CODES FOR MODELING SPEAKER-ADAPTIVE DNN-BASED SPEECH SYNTHESIS SYSTEMS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "3",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "Speech synthesis technology has a wide range of applications such as voice assistants. In recent years waveform-level synthesis systems have achieved state-of-the-art performance, as they overcome the limitations of vocoder-based synthesis systems. A range of waveform-level synthesis systems have been proposed; this paper investigates the performance of hierarchical Recurrent Neural Networks (RNNs) for speech synthesis. First, the form of network conditioning is discussed, comparing linguistic features and vocoder features from a vocoder-based synthesis system. It is found that compared with linguistic features, conditioning on vocoder features requires less data and modeling power, and yields better performance when there is limited data. By conditioning the hierarchical RNN on vocoder features, this paper develops a neural vocoder, which is capable of high quality synthesis when there is sufficient data. Furthermore, this neural vocoder is flexible, as conceptually it can map any sequence of vocoder features to speech, enabling efficient synthesizer porting to a target speaker. Subjective listening tests demonstrate that the neural vocoder outperforms a high quality baseline, and that it can change its voice to a very different speaker, given less than 15 minutes of data for fine tuning.",
        "Authors": [
            "Qingyun Dou; University of Cambridge",
            "Moquan Wan; University of Cambridge",
            "Gilles Degottex; University of Cambridge",
            "Zhiyi Ma; University of Cambridge",
            "Mark Gales; University of Cambridge"
        ],
        "Paper Title": "HIERARCHICAL RNNS FOR WAVEFORM-LEVEL SPEECH SYNTHESIS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "4",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recurrent Neural Networks (RNN) have recently proved to be effective in acoustic modeling for TTS. Various techniques such as the Maximum Likelihood Parameter Generation (MLPG) algorithm have been naturally inherited from the HMM-based speech synthesis framework. This paper investigates in which situations parameter generation and variance restoration approaches help for RNN-based TTS. We explore how their performance is affected by various factors such as the choice of the loss function, the application of regularization methods and the amount of training data. We propose an efficient way to calculate MLPG using a convolutional kernel. Our results show that the use of the L1 loss with proper regularization outperforms any system built with the conventional L2 loss and does not require to apply MLPG (which is necessary otherwise). We did not observe perceptual improvements when embedding MLPG into the acoustic model. Finally, we show that variance restoration approaches are important for cepstral features but only yield minor perceptual gains for the prediction of F0.",
        "Authors": [
            "Viacheslav Klimkov; Amazon",
            "Alexis Moinet; Amazon",
            "Adam Nadolski; Amazon",
            "Thomas Drugman; Amazon"
        ],
        "Paper Title": "PARAMETER GENERATION ALGORITHMS FOR TEXT-TO-SPEECH SYNTHESIS WITH RECURRENT NEURAL NETWORKS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "5",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We propose a learning-based filter that allows us to directly modify a waveform of synthetic speech to that of natural speech. Speech-processing systems using a vocoder framework such as statistical parametric speech synthesis and voice conversion are convenient especially for a limited data because it is possible to represent and process interpretable acoustic features over a compact space, such as the fundamental frequency and mel-cepstrum. However, the well-known problem leading the quality degradation of generated speech is an over-smoothing effect lacking some detailed structure of generated/converted acoustic features. To address this issue, we propose a synthetic-to-natural speech waveform conversion using cycle-consistent adversarial networks which doesn't require any explicit assumption about speech waveform in adversarial learning. In contrast to current techniques, since our modification is performed at the waveform level, we expect that the proposed method also enables to generate ``vocoder less'' sounding speech even if the input speech is synthesized by using the vocoder framework. The experimental results demonstrate that our proposed method achieves to 1) alleviate the over-smoothing effect of the acoustic features nevertheless the direct modification method for the waveform and 2) dramatically improve the naturalness of the generated speech sounds.",
        "Authors": [
            "Kou Tanaka; NTT corporation",
            "Takuhiro Kaneko; NTT corporation",
            "Nobukatsu Hojo; NTT corporation",
            "Hirokazu Kameoka; NTT corporation"
        ],
        "Paper Title": "SYNTHETIC-TO-NATURAL SPEECH WAVEFORM CONVERSION USING CYCLE-CONSISTENT ADVERSARIAL NETWORKS",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "6",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "End-to-end TTS model can directly take an utterance as reference, and generate speech from the text with prosody and speaker characteristics similar to the reference utterance. Ideally, the transcription of reference utterance does not need to match the text to be synthesized, so unsupervised style transfer can be achieved. However, in the previous model, because only the matched text and speech are used in training, given unmatched text and speech during testing would make the model synthesize blurry speech. In this paper, we propose to mitigate the problem by using the unmatched text and speech during training, and using the ASR accuracy of an end-to-end ASR model to guide the training procedure. The experimental results show that with the guidance of end-to-end ASR, both the ASR accuracy (objective evaluation) and the listener preference (subjective evaluation) of the speech generated by TTS model are improved. Moreover, we propose attention consistency loss as regularization, which is shown to accelerate the training.",
        "Authors": [
            "Da-Rong Liu; National Taiwan University",
            "Chi-Yu Yang; National Taiwan University",
            "Szu-Lin Wu; National Taiwan University",
            "Hung-Yi Lee; National Taiwan University"
        ],
        "Paper Title": "IMPROVING UNSUPERVISED STYLE TRANSFER IN END-TO-END SPEECH SYNTHESIS WITH END-TO-END SPEECH RECOGNITION",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "7",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "A sequence-to-sequence model is a neural network module for mapping two sequences of different lengths. The sequence-to-sequence model has three core modules: encoder, decoder, and attention. Attention is the bridge that connects the encoder and decoder modules and improves model performance in many tasks. In this paper, we propose two ideas to improve sequence-to-sequence model performance by enhancing the attention module. First, we maintain the history of the location and the expected context from several previous time-steps. Second, we apply multiscale convolution from several previous attention vectors to the current decoder state. We utilized our proposed framework for sequence-to-sequence speech recognition and text-to-speech systems. The results reveal that our proposed extension can improve performance significantly compared to a standard attention baseline.",
        "Authors": [
            "Andros Tjandra; Nara Institute of Science and Technology",
            "Sakriani Sakti; Nara Institute of Science and Technology",
            "Satoshi Nakamura; Nara Institute of Science and Technology"
        ],
        "Paper Title": "MULTI-SCALE ALIGNMENT AND CONTEXTUAL HISTORY FOR ATTENTION MECHANISM IN SEQUENCE-TO-SEQUENCE MODEL",
        "Presentation": "Invited talk, Discussion, Oral presentation, Poster session",
        "Presentation #": "8",
        "Presentation Time": "Tuesday, December 18, 14:00 - 17:00",
        "Session": "Deep Learning for Speech Synthesis",
        "Session Time": "Tuesday, December 18, 14:00 - 17:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In topic identification (topic ID) on real-world unstructured audio, an audio instance of variable topic shifts is first broken into sequential segments, and each segment is independently classified. We first present a general purpose method for topic ID on spoken segments in low-resource languages, using a cascade of universal acoustic modeling, translation lexicons to English, and English-language topic classification. Next, instead of classifying each segment independently, we demonstrate that exploring the contextual dependencies across sequential segments can provide large improvements. In particular, we propose an attention-based contextual model which is able to leverage the contexts in a selective manner. We test both our contextual and non-contextual models on four LORELEI languages, and on all but one our attention-based contextual model significantly outperforms the context-independent models.",
        "Authors": [
            "Chunxi Liu; Johns Hopkins University",
            "Matthew Wiesner; Johns Hopkins University",
            "Shinji Watanabe; Johns Hopkins University",
            "Craig Harman; Johns Hopkins University",
            "Jan Trmal; Johns Hopkins University",
            "Najim Dehak; Johns Hopkins University",
            "Sanjeev Khudanpur; Johns Hopkins University"
        ],
        "Paper Title": "LOW-RESOURCE CONTEXTUAL TOPIC IDENTIFICATION ON SPEECH",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Personal digital assistants must display the output from the speech recognizer in a compact and readable representation. The process of transforming sequences from spoken words to written text is called inverse text normalization (ITN). In this paper, we present a ranking based approach to ITN that incorporates predicative information from various neural-net LSTM and n-gram models to select the best written text to display. Our approach ranks the written text candidates, generated by applying weighted FSTs to the spoken words, using a gradient boosted decision tree ensemble (GBDT). The ranker achieves a 18.48% relative reduction in word error rate over an unweighted FST system. Further, our two-stage approach allows us to decouple speech recognition from ITN and gives us greater flexibility in system configuration, since the written-form can vary by domain.",
        "Authors": [
            "Issac Alphonso; Microsoft",
            "Nick Kibre; Microsoft",
            "Tasos Anastasakos; Microsoft"
        ],
        "Paper Title": "RANKING APPROACH TO COMPACT TEXT REPRESENTATION FOR PERSONAL DIGITAL ASSISTANTS",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Large scale Natural Language Understanding (NLU) systems are typically trained on large quantities of data, requiring a fast and scalable training strategy. A typical design for NLU systems consists of domain-level NLU modules (domain classifier, intent classifier and named entity recognizer). Hypotheses (NLU interpretations consisting of various intent+slot combinations) from these domain specific modules are typically aggregated with another downstream component. The re-ranker integrates outputs from domain-level recognizers, returning a scored list of cross domain hypotheses. An ideal re-ranker will exhibit the following two properties: (a) it should prefer the most relevant hypothesis for the given input as the top hypothesis and, (b) the interpretation scores corresponding to each hypothesis produced by the re-ranker should be calibrated. Calibration allows the final NLU interpretation score to be comparable across domains. We propose a novel re-ranker strategy that addresses these aspects, while also maintaining domain specific modularity. We design optimization loss functions for such a modularized re-ranker and present results on decreasing the top hypothesis error rate as well as maintaining the model calibration. We also experiment with an extension involving training the domain specific re-rankers on datasets curated independently by each domain to allow further asynchronization.",
        "Authors": [
            "Chengwei Su; Amazon",
            "Rahul Gupta; Amazon",
            "Shankar Ananthakrishnan; Amazon",
            "Spyros Matsoukas; Amazon"
        ],
        "Paper Title": "A Re-ranker Scheme for Integrating Large Scale NLU models",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Typical spoken language understanding systems provide narrow semantic parses using a domain-specific ontology. The parses contain intents and slots that are directly consumed by downstream domain applications. In this work we discuss expanding such systems to handle compound entities and intents by introducing a domain-agnostic shallow parser that handles linguistic coordination. We show that our model for parsing coordination learns domain-independent and slot-independent features and is able to segment conjunct boundaries of many different phrasal categories. We also show that using adversarial training can be effective for improving generalization across different slot types for coordination parsing.",
        "Authors": [
            "Sanchit Agarwal; Amazon",
            "Rahul Goel; Amazon",
            "Tagyoung Chung; Amazon",
            "Abhishek Sethi; Amazon",
            "Arindam Mandal; Amazon",
            "Spyros Matsoukas; Amazon"
        ],
        "Paper Title": "PARSING COORDINATION FOR SPOKEN LANGUAGE UNDERSTANDING",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Quaternion convolutional neural networks (QCNN) are powerful architectures to learn and model external dependencies that exist between neighbor features of an input vector, and internal latent dependencies within the feature. This paper proposes to evaluate the effectiveness of the QCNN on a realistic theme identification task of spoken telephone conversations between agents and customers from the call center of the Paris transportation system (RATP). We show that QCNNs are more suitable than real-valued CNN to process multidimensional data and to code internal dependencies. Indeed, real-valued CNNs deal with both internal and external relations at the same level since components of an entity are processed independently. Experimental evidence is provided that the proposed QCNN architecture always outperforms real-valued equivalent CNN models in the theme identification task of the DECODA corpus. It is also shown that QCNN accuracy results are the best achieved so far on this task, while reducing by a factor of 4 the number of model parameters.",
        "Authors": [
            "Titouan Parcollet; Université d'Avignon et des pays du Vaucluse",
            "Mohamed Morchid; Université d'Avignon et des pays du Vaucluse",
            "Georges Linarès; Université d'Avignon et des pays du Vaucluse",
            "Renato De Mori; McGill University"
        ],
        "Paper Title": "Quaternion Convolutional Neural Networks for Theme Identification of Telephone Conversations",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents. Usually, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propagation, sub-optimal tuning of ASR systems in regards to the final task, reduced space search at the ASR output level,...) and it is known that more integrated approaches outperform sequential ones, when they can be applied. In this paper, we explore an end-to-end approach that directly extracts named entities from speech, though a unique neural architecture. On a such way, a joint optimization is possible for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaigns. The results are promising since this end-to-end approach provides similar results (F-measure=0.66 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.64). Last, we also explore this approach applied to semantic concept extraction, through a slot filling task known as a spoken language understanding problem, and also observe an improvement in comparison to a pipeline approach.",
        "Authors": [
            "Sahar Ghannay; University of Le Mans",
            "Antoine Caubrière; University of Le Mans",
            "Yannick Estève; University of Le Mans",
            "Nathalie Camelin; University of Le Mans",
            "Edwin Simonnet; University of Le Mans",
            "Antoine Laurent; University of Le Mans",
            "Emmanuel Morin; University of Nantes"
        ],
        "Paper Title": "End-to-end named entity and semantic concept extraction from speech",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Emphasis is an important factor of human speech that helps convey emotion and the essential information of utterances. Recently, studies have been conducted on speech-to-speech translation to preserve the emphasis information from the source language to the target language. However, since different cultures have various ways of expressing emphasis, just considering the acoustic-to-acoustic features emphasis translation may not always reflect the experiences of users. On the other hand, emphasis can be expressed at various levels in both text and speech. But it remains unclear how we communicate emphasis in a different form acoustic/linguistic) with different levels and whether we can perceive the difference between different levels of emphasis or observe the similarity of the same emphasis levels in both text and speech forms. In this paper, we conducted analyses on human perception of emphasis with both speech and text clues through crowd-sourced evaluations. The results indicate that although participants can distinguish among emphasis levels and perceive the same emphasis level between speech and text, many ambiguities still exist at certain emphasis levels. Thus, our result provides insight into what needs to be handled during the emphasis translation process.",
        "Authors": [
            "Quoc Truong Do; Nara Institute of Science and Technology",
            "Sakriani Sakti; Nara Institute of Science and Technology/AIP",
            "Satoshi Nakamura; Nara Institute of Science and Technology/AIP"
        ],
        "Paper Title": "TOWARD MULTI-FEATURES EMPHASIS SPEECH TRANSLATION: ASSESSMENT OF HUMAN EMPHASIS PRODUCTION AND PERCEPTION WITH SPEECH AND TEXT CLUES",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Ordering products and services through virtual agents is possible but suffers limitations on the kind of ordering that is possible or on the naturalness of the conversation. We address these limitations by collecting a corpus of human-human dialogs in the food ordering domain. We create a food focused annotation scheme that is tailored for this corpus but customizable for other applications. After annotating the corpus, we find corpus characteristics that may make it more natural, such as complexity of food item mentions and use of multiple intent utterances. Furthermore, we train and evaluate preliminary statistical item and intent models using the annotated corpus.",
        "Authors": [
            "John Chen; Interactions, LLC.",
            "Rashmi Prasad; Interactions, LLC.",
            "Svetlana Stoyanchev; Interactions, LLC.",
            "Ethan Selfridge; Interactions, LLC.",
            "Srinivas Bangalore; Interactions, LLC.",
            "Michael Johnston; Interactions, LLC."
        ],
        "Paper Title": "CORPUS AND ANNOTATION TOWARDS NLU FOR CUSTOMER ORDERING DIALOGS",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Representation learning is an essential problem in a wide range of applications and it is important for performing downstream tasks successfully. In this paper, we propose a new model that learns coupled representations of domains, intents, and slots by taking advantage of their hierarchical dependency in a Spoken Language Understanding system. Our proposed model learns the vector representation of intents based on the slots tied to these intents by aggregating the representations of the slots. Similarly, the vector representation of a domain is learned by aggregating the representations of the intents tied to a specific domain. To the best of our knowledge, it is the first approach to jointly learning the representations of domains, intents, and slots using their hierarchical relationships. The experimental results demonstrate the effectiveness of the representations learned by our model, as evidenced by improved performance on the contextual cross-domain reranking task.",
        "Authors": [
            "Jihwan Lee; Amazon",
            "Dongchan Kim; Amazon",
            "Ruhi Sarikaya; Amazon",
            "Young-Bum Kim; Amazon"
        ],
        "Paper Title": "COUPLED REPRESENTATION LEARNING FOR DOMAINS, INTENTS AND SLOTS IN SPOKEN LANGUAGE UNDERSTANDING",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Conventional spoken language understanding systems consist of two main components: an automatic speech recognition module that converts audio to a transcript, and a natural language understanding module that transforms the resulting text (or top N hypotheses) into a set of domains, intents, and arguments. These modules are typically optimized independently. In this paper, we formulate audio to semantic understanding as a sequence-to-sequence problem. We propose and compare various encoder-decoder based approaches that optimize both modules jointly, in an end-to-end manner. Evaluations on a real-world task show that 1) having an intermediate text representation is crucial for the quality of the predicted semantics, especially the intent arguments and 2) jointly optimizing the full system improves overall accuracy of prediction. Compared to independently trained models, our best jointly trained model achieves similar intent prediction F1 scores, but improves argument word error rate by 18% relative.",
        "Authors": [
            "Parisa Haghani; Google",
            "Arun Narayanan; Google",
            "Michiel Bacchiani; Google",
            "Galen Chuang; Google",
            "Neeraj Gaur; Google",
            "Pedro Moreno; Google",
            "Rohit Prabhavalkar; Google",
            "Zhongdi Qu; Google",
            "Austin Waters; Google"
        ],
        "Paper Title": "FROM AUDIO TO SEMANTICS: APPROACHES TO END-TO-END SPOKEN LANGUAGE UNDERSTANDING",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "Automatic speech recognition (ASR) and natural language understanding are critical components of spoken language understanding (SLU) systems. One obstacle to providing services with SLU systems in multiple languages is the cost associated with acquiring all of the language-specific resources required for ASR in each language. Modeling graphemes eliminates the need to obtain a pronunciation dictionary which maps from speech sounds to words and is one way to reduce ASR resource dependencies when rapidly developing ASR in new languages. However, little is known about the downstream impact on SLU task performance when selecting graphemes as the acoustic modeling unit. This work investigates acoustic modeling for the ASR component of an SLU system using grapheme-based approaches together with convolutional and recurrent neural network architectures. We evaluate both ASR word accuracy and spoken utterance classification (SUC) accuracy for English, Italian and Spanish language tasks and find that it is possible to achieve SUC accuracy that is comparable to conventional phoneme-based systems which leverage a pronunciation dictionary.",
        "Authors": [
            "Ryan Price; Interactions, LLC.",
            "Bhargav Srinivas Ch; Interactions, LLC.",
            "Surbhi Singhal; Interactions, LLC.",
            "Srinivas Bangalore; Interactions, LLC."
        ],
        "Paper Title": "INVESTIGATING THE DOWNSTREAM IMPACT OF GRAPHEME-BASED ACOUSTIC MODELING ON SPOKEN UTTERANCE CLASSIFICATION",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "Spoken Language Understanding",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Spoken language understanding:"
    },
    {
        "Abstract": "In this paper, we propose to use high-degree features using polynomial expansion to improve the discrimination performance of Deep Neural Network (DNN) based acoustic model. Thanks to the success of DNNs for high-dimensional non-linear classification problems, various acoustic information can be represented in high dimensional features, and the non-linear characteristics of speech signal can be robustly generalized in DNN-based acoustic models. Even though it is not clear how DNNs to solve the classification problem, the use of high-dimensional features is based on a well-known knowledge that it helps separability of patters. There is another well-known knowledge that high-degree features increase linear separability of non-linear input features. However, there is little work to exploit high-degree features. Therefore, in this work, we investigate the high-degree features to improve the performance of DNN-based acoustic model further. In this work, the proposed approach was evaluated on a Wall Street Journal (WSJ) speech recognition domain. The proposed method achieved up to 21.8% error reduction rate for the Eval92 test set by reducing the word error rate from 4.82% to 3.77% when using degree-2 polynomial expansion.",
        "Authors": [
            "Hoon Chung; Electronics and Telecommunications Research Institute",
            "Sung Joo Lee; Electronics and Telecommunications Research Institute",
            "Jeon Gue Park; Electronics and Telecommunications Research Institute"
        ],
        "Paper Title": "HIGH-DEGREE FEATURE FOR DEEP NEURAL NETWORK BASED ACOUSTIC MODEL",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In recent years, robust automatic speech recognition (ASR) has greatly taken benefit from the use of neural networks for acoustic modeling, although performance still degrades in severe noise conditions. Based on the previous success of models using convolutional and subsequent bidirectional long short-term memory (BLSTM) layers in the same network, we propose to use a densely connected convolutional network (DenseNet) as the first part of such a model, while the second is a BLSTM network. A particular contribution of our work is that we modify the DenseNet topology to become a kind of feature extractor for the subsequent BLSTM network operating on whole speech utterances. We evaluate our model on the 6-channel task of CHiME-4, and are able to consistently outperform a top-performing baseline based on wide residual networks and BLSTMs providing a 2.4% relative WER reduction on the real test set.",
        "Authors": [
            "Maximilian Strake; Technische Universität Braunschweig",
            "Pascal Behr; Technische Universität Braunschweig",
            "Timo Lohrenz; Technische Universität Braunschweig",
            "Tim Fingscheidt; Technische Universität Braunschweig"
        ],
        "Paper Title": "DENSENET BLSTM FOR ACOUSTIC MODELING IN ROBUST ASR",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Dysarthria is a neurological speech impairment, which usually results in the loss of motor speech control due to muscular atrophy and incoordination of the articulators. As a result the speech becomes less intelligible and difficult to model by machine learning algorithms due to inconsistencies in the acoustic signal and data sparseness. This paper presents phase-based feature representations for dysarthric speech that are exploited in the group delay spectrum. Such representations are found to be better suited to characterising the resonances of the vocal tract, exhibit better phone discrimination capabilities in dysarthric signals and consequently improve ASR performance. All the experiments were conducted using the UASPEECH corpus and significant ASR gains are reported using phase-based cepstral features in comparison to the standard MFCCs irrespective of the severity of the condition.",
        "Authors": [
            "Siddharth Sehgal; University of Sheffield",
            "Stuart Cunningham; University of Sheffield",
            "Phil Green; University of Sheffield"
        ],
        "Paper Title": "PHASE-BASED FEATURE REPRESENTATIONS FOR IMPROVING RECOGNITION OF DYSARTHRIC SPEECH",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper, we propose a novel training strategy for deep neural network (DNN) based small-footprint acoustic models. The accuracy of DNN-based automatic speech recognition (ASR) systems can be greatly improved by leveraging large amounts of data to improve the level of expression. DNNs use many parameters to enhance recognition performance. Unfortunately, resource-constrained local devices are unable to run complex DNN-based ASR systems. For building compact acoustic models, the knowledge distillation (KD) approach is often used. KD uses a large, well-trained model that outputs target labels to train a compact model. However, the standard KD cannot fully utilize the large model outputs to train compact models because the soft logits provide only rough information. We assume that the large model must give more useful hints to the compact model. We propose an advanced KD that uses mean squared error to minimize the discrepancies between the final hidden layer outputs. We evaluate our proposal on recorded speech data sets assuming car- and home-use scenarios, and show that our models achieve lower character error rates than the conventional KD approach or from-scratch training on computation resource-constrained devices.",
        "Authors": [
            "Takafumi Moriya; NTT Corporation",
            "Hiroki Kanagawa; NTT Corporation",
            "Kiyoaki Matsui; NTT Corporation",
            "Takaaki Fukutomi; NTT Corporation",
            "Yusuke Shinohara; NTT Corporation",
            "Yoshikazu Yamaguchi; NTT Corporation",
            "Manabu Okamoto; NTT Corporation",
            "Yushi Aono; NTT Corporation"
        ],
        "Paper Title": "EFFICIENT BUILDING STRATEGY WITH KNOWLEDGE DISTILLATION FOR SMALL-FOOTPRINT ACOUSTIC MODELS",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Non-native speech causes automatic speech recognition systems to degrade in performance. Past strategies to address this challenge have considered model adaptation, accent classification with a model selection, alternate pronunciation lexicon, etc. In this study, we consider a recurrent neural network (RNN) with connectionist temporal classification (CTC) cost function trained on multi-accent English data including US (Native), Indian and Hispanic accents. We exploit dark knowledge from a model trained with the multi-accent data to train student models under the guidance of both a teacher model and CTC cost of target transcription. Transferring knowledge from a single RNN-CTC trained model toward a student model, yields better performance than the stand-alone teacher model. Since the outputs of different trained CTC models are not necessarily aligned, it is not possible to simply use an ensemble of CTC teacher models. To address this problem, we train accent specific models under the guidance of a single multi-accent teacher, which results in having multiple aligned and trained CTC models. Furthermore, we train a student model under the supervision of the accent-specific teachers, resulting in an even further complementary model, which achieves +20.1% relative Character Error Rate (CER) reduction compared to the baseline trained without any teacher.",
        "Authors": [
            "Shahram Ghorbani; University of Texas at Dallas",
            "Ahmet E. Bulut; University of Texas at Dallas",
            "John H.L. Hansen; University of Texas at Dallas"
        ],
        "Paper Title": "ADVANCING MULTI-ACCENTED LSTM-CTC SPEECH RECOGNITION USING A DOMAIN SPECIFIC STUDENT-TEACHER LEARNING PARADIGM",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Despite recent progress in developing Large Vocabulary Continuous Speech Recognition Systems (LVCSR), these systems suffer from Out-Of-Vocabulary words (OOV). In many cases, the OOV words are Proper Nouns (PNs). The correct recognition of PNs is essential for broadcast news, audio indexing, etc. In this article, we address the problem of OOV PN retrieval in the framework of broadcast news LVCSR. We focused on dynamic (document dependent) extension of LVCSR lexicon. To retrieve relevant OOV PNs, we propose to use a very large multi-topic text corpus: Wikipedia. This corpus contains a huge number of PNs. These PNs are grouped in semantically similar classes using word embedding. We use a two-step approach: first, we select OOV pertinent classes with a multi-class Deep Neural Network (DNN). Secondly, we rank the OOVs of the selected classes. The experiments on French broadcast news show that the Bi-GRU model outperforms other studied models. Speech recognition experiments demonstrate the effectiveness of the proposed methodology.",
        "Authors": [
            "Badr Abdullah; LORIA/INRIA",
            "Irina Illina; LORIA/INRIA",
            "Dominique Fohr; LORIA/INRIA"
        ],
        "Paper Title": "DYNAMIC EXTENSION OF ASR LEXICON USING WIKIPEDIA DATA",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We present our work on improving the numerator graph for discriminative training using the lattice-free maximum mutual information (MMI) criterion. Specifically, we propose a scheme for creating unconstrained numerator graphs by removing time constraints from the baseline numerator graphs. This leads to much smaller graphs and therefore faster preparation of training supervisions. By testing the proposed unconstrained supervisions using factorized time-delay neural network (TDNN) models, we observe 0.5\\% to 2.6\\% relative improvement over the state-of-the-art word error rates on various large-vocabulary speech recognition databases.",
        "Authors": [
            "Hossein Hadian; Sharif University of Technology",
            "Daniel Povey; Johns Hopkins University",
            "Hossein Sameti; Sharif University of Technology",
            "Jan Trmal; Johns Hopkins University",
            "Sanjeev Khudanpur; Johns Hopkins University"
        ],
        "Paper Title": "IMPROVING LF-MMI USING UNCONSTRAINED SUPERVISIONS FOR ASR",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recurrent neural networks have been the dominant models for many speech and language processing tasks. However, we understand little about the behavior and the class of functions recurrent networks can realize. Moreover, the heuristics used during training complicate the analyses. In this paper, we study recurrent networks' ability to learn long-term dependency in the context of speech recognition. We consider two decoding approaches, online and batch decoding, and show the classes of functions to which the decoding approaches correspond. We then draw a connection between batch decoding and a popular training approach for recurrent networks, truncated backpropagation through time. Changing the decoding approach restricts the amount of past history recurrent networks can use for prediction, allowing us to analyze their ability to remember. Empirically, we utilize long-term dependency in subphonetic states, phonemes, and words, and show how the design decisions, such as the decoding approach, lookahead, context frames, and consecutive prediction, characterize the behavior of recurrent networks. Finally, we draw a connection between Markov processes and vanishing gradients. These results have implications for studying the long-term dependency in speech data and how these properties are learned by recurrent networks.",
        "Authors": [
            "Hao Tang; Massachusetts Institute of Technology",
            "James Glass; Massachusetts Institute of Technology"
        ],
        "Paper Title": "ON TRAINING RECURRENT NETWORKS WITH TRUNCATED BACKPROPAGATION THROUGH TIME IN SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Despite rapid advances in speech recognition, current models remain brittle to superficial perturbations to their inputs. Small amounts of noise can destroy the performance of an otherwise state-of-the-art model. To harden models against background noise, practitioners often perform data augmentation, adding artificially-noised examples to the training set, carrying over the original label. In this paper, we hypothesize that a clean example and its superficially perturbed counterparts shouldn't merely map to the same class--- they should map to the same representation. We propose invariant-representation-learning (IRL): At each training iteration, for each training example, we sample a noisy counterpart. We then apply a penalty term to coerce matched representations at each layer (above some chosen layer). Our key results, demonstrated on the Librispeech dataset are the following: (i) IRL significantly reduces character error rates (CER) on both 'clean' (3.3% vs 6.5%) and 'other' (11.0% vs 18.1%) test sets; (ii) on several out-of-domain noise settings (different from those seen during training) IRL's benefits are even more pronounced. Careful ablations confirm that our results are not simply due to shrinking activations at the chosen layers.",
        "Authors": [
            "Davis Liang; Amazon AI",
            "Zhiheng Huang; Amazon AI",
            "Zachary Lipton; Carnegie Mellon University"
        ],
        "Paper Title": "Learning Noise-Invariant Representations for Robust Speech Recognition",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Conventional acoustic models for automatic speech recognition (ASR) are usually constructed from sub-word unit (e.g., context-dependent phoneme, grapheme, wordpiece etc.). Recent studies demonstrate that connectionist temporal classification (CTC) based acoustic-to-word (A2W) models are also promising for ASR. Such structures have drawn increasing attention as they can directly target words as output units, which simplify ASR pipeline by avoiding additional pronunciation lexicon, or even language model. In this study, we systematically explore to use word as acoustic modeling unit for conversational speech recognition. By replacing senone alignment with word alignment in a convolutional bidirectional LSTM architecture and employing a lexicon-free weighted finite-state transducer (WFST) based decoding, we greatly simplify conventional hybrid speech recognition system. On Hub5-2000 Switchboard/CallHome test sets with 300-hour training data, we achieve a WER that is close to the senone based hybrid systems with a WFST based decoding.",
        "Authors": [
            "Chunlei Zhang; The University of Texas at Dallas",
            "Chengzhu Yu; Tencent AI Lab",
            "Chao Weng; Tencent AI Lab",
            "Jia Cui; Tencent AI Lab",
            "Dong Yu; Tencent AI Lab"
        ],
        "Paper Title": "An Exploration of Directly Using Word as Acoustic Modeling Unit for Speech Recognition",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "A new whole-sentence language model - neural trans-dimensional random field language model (neural TRF LM), where sentences are modeled as a collection of random fields, and the potential function is defined by a neural network, has been introduced and successfully trained by noise-contrastive estimation (NCE). In this paper, we extend NCE and propose dynamic noise-contrastive estimation (DNCE) to solve the two problems observed in NCE training. First, a dynamic noise distribution is introduced and trained simultaneously to converge to the data distribution. This helps to significantly cut down the noise sample number used in NCE and reduce the training cost. Second, DNCE discriminates between sentences generated from the noise distribution and sentences generated from the interpolation of the data distribution and the noise distribution. This alleviates the overfitting problem caused by the sparseness of the training set. With DNCE, we can successfully and efficiently train neural TRF LMs on large corpus (about 0.8 billion words) with large vocabulary (about 568 K words). Neural TRF LMs perform as good as LSTM LMs with less parameters and being 5x~114x faster in rescoring sentences. Interpolating neural TRF LMs with LSTM LMs and n-gram LMs can further reduce the error rates.",
        "Authors": [
            "Bin Wang; Tsinghua University",
            "Zhijian Ou; Tsinghua University"
        ],
        "Paper Title": "Improved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "The very deep neural network has recently been proposed for speech recognition and achieves significant performance. It has excellent potential for integration with end-to-end (E2E) training. Connectionist temporal classification (CTC) has shown great potential in E2E acoustic modeling. In this study, we investigate deep architectures and techniques which are suitable for CTC-based acoustic modeling. We propose a very deep residual time-delay CTC neural network (VResTD-CTC). How to select a suitable deep architecture optimized with the CTC objective function is crucial for obtaining the state of the art performance. Excellent performances can be obtained by selecting deep architecture for non-E2E ASR systems modeling with tied-triphone states. However, these optimized structures do not guarantee to achieve better or comparable performances on E2E (e.g., CTC-based) systems modeling with dynamic acoustic units. For solving this problem and further leveraging the system performance, we introduce the vertical-attention mechanism to reweight the residual blocks at each time step. Speech recognition experiments show our proposed model significantly outperforms the DNN and LSTM-based (both bidirectional and unidirectional) CTC baseline models.",
        "Authors": [
            "Sheng Li; National Institute of Information and Communications Technology",
            "Xugang Lu; National Institute of Information and Communications Technology",
            "Ryoichi Takashima; National Institute of Information and Communications Technology",
            "Peng Shen; National Institute of Information and Communications Technology",
            "Tatsuya Kawahara; National Institute of Information and Communications Technology (NICT) / Kyoto University",
            "Hisashi Kawai; National Institute of Information and Communications Technology"
        ],
        "Paper Title": "IMPROVING VERY DEEP TIME-DELAY NEURAL NETWORK WITH VERTICAL-ATTENTION FOR EFFECTIVELY TRAINING CTC-BASED ASR SYSTEMS",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Wednesday, December 19, 10:00 - 12:00",
        "Session": "ASR I",
        "Session Time": "Wednesday, December 19, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of English speaking proficiency for non-native speakers. Rhetorical Structure Theory (RST) has been commonly used in the analysis of discourse organization of written texts; however, limited research has been conducted to date on RST annotation and parsing of spoken language, in particular, non-native spontaneous speech. Due to the fact that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spoken language, we initiated a research effort to first obtain RST annotations on non-native spoken responses from a standardized assessment of academic English proficiency. Afterwards, based on the annotations obtained, automatic parsers were built to process non-native spontaneous speech. Finally, a set of effective features were extracted from both manually annotated and automatically generated RST trees to evaluate the discourse structure of non-native spontaneous speech, and then employed to further improve the validity of an automated speech scoring system.",
        "Authors": [
            "Xinhao Wang; Educational Testing Service",
            "Binod Gyawali; Educational Testing Service",
            "James V. Bruno; Educational Testing Service",
            "Hillary R. Molloy; Educational Testing Service",
            "Keelan Evanini; Educational Testing Service",
            "Klaus Zechner; Educational Testing Service"
        ],
        "Paper Title": "DISCOURSE MODELING OF NON-NATIVE SPONTANEOUS SPEECH USING THE RHETORICAL STRUCTURE THEORY FRAMEWORK",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Spoken language corpora:"
    },
    {
        "Abstract": "When translating speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned references for Fisher Spanish-English for this task. We compare how different architectures handle disfluencies, and provide a baseline for removing disfluencies in end-to-end translation.",
        "Authors": [
            "Elizabeth Salesky; Carnegie Mellon University",
            "Susanne Burger; Carnegie Mellon University",
            "Jan Niehues; Karlsruhe Institute of Technology",
            "Alex Waibel; Carnegie Mellon University"
        ],
        "Paper Title": "TOWARDS FLUENT TRANSLATIONS FROM DISFLUENT SPEECH",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Spoken language corpora:"
    },
    {
        "Abstract": "Speech processing, automatic speech and speaker recognition are the major area of interests in the field of computational linguistics. Research and development of computer and human interaction, forensic technologies and dialogue systems have been the motivating factor behind this interest. In this paper, JSpeech is introduced, a multi-lingual corpus. This corpus contains 1332 hours of conversational speech from 47 different languages. This corpus can be used in a variety of studies, created from 106 public chat group the effect of language variability on the performance of speaker recognition systems and automatic language detection. To this end, we include speaker verification results obtained for this corpus using a state of the art method based on 3D convolutional neural network.",
        "Authors": [
            "Ali Janalizadeh Choobbasti; Amirkabir University of Technology",
            "Mohammad Erfan Gholamian; Amirkabir University of Technology",
            "Amir Vaheb; Miras Technologies International",
            "Saeid Safavi; University of Surrey"
        ],
        "Paper Title": "JSpeech: A Multi-lingual Conversational Speech Corpus",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Spoken language corpora:"
    },
    {
        "Abstract": "In human-human conversations, listeners often convey intentions to speakers through feedback consisting of reflexive short responses. The speakers recognize these intentions and change the conversational plans to make communication more efficient. These functions are expected to be effective in human-system conversations also; however, there is only a few systems using these functions or a research corpus including such functions. We created a corpus that consists of users' short responses to an actual conversation system and developed a model for recognizing the intention of these responses. First, we categorized the intention of feedback that affects the progress of conversations. We then collected 15604 short responses of users from 2060 conversation sessions using our news-delivery conversation system. Twelve annotators labeled each utterance based on intention through a listening test. We then designed our deep-neural-network-based intention recognition model using the collected data. We found that feedback in the form of questions, which is the most frequently occurring expression, was correctly recognized and contributed to the efficiency of the conversation system.",
        "Authors": [
            "Katsuya Yokoyama; Waseda University",
            "Hiroaki Takatsu; Waseda University",
            "Hiroshi Honda; Honda R&D Co.,Ltd",
            "Shinya Fujie; Chiba Institute of Technology",
            "Tetsunori Kobayashi; Waseda University"
        ],
        "Paper Title": "INVESTIGATION OF USERS' SHORT RESPONSES IN ACTUAL CONVERSATION SYSTEM AND AUTOMATIC RECOGNITION OF THEIR INTENTIONS",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Spoken language corpora:"
    },
    {
        "Abstract": "Word embedding or Word2Vec has been successful in offering semantics for text words learned from the context of words. Audio Word2Vec was shown to offer phonetic structures for spoken words (signal segments for words) learned from signals within spoken words. This paper proposes a two-stage framework to perform phonetic-and-semantic embedding on spoken words considering the context of the spoken words. Stage 1 performs phonetic embedding with speaker characteristics disentangled. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings. In general, phonetic structure and semantics inevitably disturb each other. For example the words \"brother\" and \"sister\" are close in semantics but very different in phonetic structure, while the words \"brother\" and \"bother\" are in the other way around. But phonetic-and-semantic embedding is attractive, as shown in the initial experiments on spoken document retrieval. Not only spoken documents including the spoken query can be retrieved based on the phonetic structures, but spoken documents semantically related to the query but not including the query can also be retrieved based on the semantics.",
        "Authors": [
            "Yi-Chen Chen; National Taiwan University",
            "Sung-Feng Huang; National Taiwan University",
            "Chia-Hao Shen; National Taiwan University",
            "Hung-yi Lee; National Taiwan University",
            "Lin-shan Lee; National Taiwan University"
        ],
        "Paper Title": "PHONETIC-AND-SEMANTIC EMBEDDING OF SPOKEN WORDS WITH APPLICATIONS IN SPOKEN CONTENT RETRIEVAL",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Spoken document retrieval:"
    },
    {
        "Abstract": "Reading comprehension by machine has been widely studied, but machine comprehension of spoken content is still a less investigated problem. In this paper, we release Open-Domain Spoken Question Answering Dataset (ODSQA) with more than three thousand questions. To the best of our knowledge, this is the largest real SQA dataset. On this dataset, we found that ASR errors have catastrophic impact on SQA. To mitigate the effect of ASR errors, subword units are involved, which brings consistent improvements over all the models. We further found that data augmentation on text-based QA training examples can improve SQA.",
        "Authors": [
            "Chia-Hsuan Lee; National Taiwan University",
            "Shang-Ming Wang; National Taiwan University",
            "Huan-Cheng Chang; National Taiwan University",
            "Hung-Yi Lee; National Taiwan University"
        ],
        "Paper Title": "ODSQA: OPEN-DOMAIN SPOKEN QUESTION ANSWERING DATASET",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Question answering from speech:"
    },
    {
        "Abstract": "Automatic assessment of spoken language proficiency is a sought-after technology. These systems often need to handle the operating scenario where candidates have a skill level or first language which was not encountered during the training stage. For high stakes tests it is necessary for those systems to have good grading performance when the candidate is from the same population as those contained in the training set, and they should know when they are likely to perform badly in the case when the candidate is not from the same population as the ones contained in training set. This paper focuses on using Deep Density Networks to yield auto-marking confidence. Firstly, we explore the benefits of parametrising either a predictive distribution or a posterior distribution over the parameters of the model likelihood and obtaining the predictive distribution via marginalisation. Secondly, we investigate how it is possible to act on the parametrised density in order to explicitly teach the model to have low confidence in areas of the observation space where there is no training data by assigning confidence scores to artificially generated data. Lastly, we compare the capabilities of Factor Analysis, Variational Auto-Encodes, and Wasserstein Generative Adversarial Networks to generate artificial data.",
        "Authors": [
            "Marco Del Vecchio; University of Cambridge",
            "Andrey Malinin; University of Cambridge",
            "Mark Gales; University of Cambridge"
        ],
        "Paper Title": "Improved Auto-Marking Confidence for Spoken Language Assessment",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Evaluation methodologies:"
    },
    {
        "Abstract": "Many applications designed to assess and improve oral reading fluency use automated speech recognition (ASR) to provide feedback to students, teachers, and parents. Most such applications rely on a distributed architecture with the speech recognition component located in the cloud. For interactive applications, this approach requires a reliable Internet connection that may not always be available. We investigate whether on-device ASR can be used for a virtual reading companion using recordings obtained from children both in a controlled environment and in the field. Our limited evaluation makes us cautiously optimistic about the feasibility of using on-device ASR for our application.",
        "Authors": [
            "Anastassia Loukina; Educational Testing Service",
            "Nitin Madnani; Educational Testing Service",
            "Beata Beigman Klebanov; Educational Testing Service",
            "Abhinav Misra; Educational Testing Service",
            "Georgi Angelov; Astea Solutions",
            "Ognjen Todic; Keen Research"
        ],
        "Paper Title": "Evaluating On-device ASR on Field Recordings from an Interactive Reading Companion",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Evaluation methodologies: Educational:"
    },
    {
        "Abstract": "This paper investigates DNN-based scoring techniques when they are applied to two tasks related to foreign language education. One is a conventional task, which attempts to predict a language learner's overall proficiency of oral communication. For this aim, learners' shadowing utterances are assessed automatically. The other is a very new and novel task, which attempts to predict intelligibility or comprehensibility of a learner's pronunciation. In this task, native listeners' responsive shadowings are assessed. For both the tasks, similar technical frameworks are tested, where DNN-based phoneme posteriors, posteriogram-based DTW scores, ASR-based accuracies, shadowing latencies, etc are used to train regression models, which aim to predict manually rated scores. Experiments show that, in both the tasks, the correlation between the DNN-based predicted scores and the averaged human scores is higher than or at least comparable to the averaged correlation between the scores of human raters. This fact clearly indicates that our proposed automatic rating module can be introduced to language education as another human rater.",
        "Authors": [
            "Suguru Kabashima; The University of Tokyo",
            "Yuusuke Inoue; The University of Tokyo",
            "Daisuke Saito; The University of Tokyo",
            "Nobuaki Minematsu; The University of Tokyo"
        ],
        "Paper Title": "DNN-BASED SCORING OF LANGUAGE LEARNERS' PROFICIENCY USING LEARNERS' SHADOWINGS AND NATIVE LISTENERS' RESPONSIVE SHADOWINGS",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Evaluation methodologies: Educational:"
    },
    {
        "Abstract": "We present a neural network approach to the automated assessment of non-native spontaneous speech in a listen and speak task. An attention-based Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is used to learn the relations (scoring rubrics) between the spoken responses and their assigned scores. Each prompt (listening material) is encoded as a vector in a low-dimensional space and then employed as a condition of the inputs of the attention LSTM-RNN. The experimental results show that our approach performs as well as the strong baseline of a Support Vector Regressor (SVR) using content-related features, i.e., a correlation of r = 0.806 with holistic proficiency scores provided by humans, without doing any feature engineering. The prompt-encoded vector improves the discrimination between the high-scoring sample and low-scoring sample, and it is more effective in grading responses to unseen prompts, which have no corresponding responses in the training set.",
        "Authors": [
            "Yao Qian; Educational Testing Service",
            "Rutuja Ubale; Educational Testing Service",
            "Matthew Mulholland; Educational Testing Service",
            "Keelan Evanini; Educational Testing Service",
            "Xinhao Wang; Educational Testing Service"
        ],
        "Paper Title": "A PROMPT-AWARE NEURAL NETWORK APPROACH TO CONTENT-BASED SCORING OF NON-NATIVE SPONTANEOUS SPEECH",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Evaluation methodologies: Educational:"
    },
    {
        "Abstract": "Speech based diagnosis-aid tools for depression typically depend on few and small datasets, that are expensive to collect. The limited availability of training data poses a limitation to the quality that these systems can achieve. An unexplored alternative for large scale source of data are vlogs collected from online multimedia repositories. Along with the automation of the mining process, it is necessary to automate the labeling process too. In this work, we propose a framework to automatically label a corpus of in-the-wild vlogs of possibly depressed subjects, and we estimate the quality of the predicted labels, without ever having access to a ground truth for the majority of the corpus. The framework uses a small subset to train a model and estimate the labels for the remainder of the corpus. Then, using the predicted labels, we train a noisy model and attempt to reconstruct the labels of the original labeled subset. We hypothesize that the quality of the estimated labels for the unlabelled subset of the corpus is correlated to the quality of the label reconstruction of the labeled subset. The results of the bi-modal experiment using in-the-wild data are compared with the ones obtained using controlled data.",
        "Authors": [
            "Joana Correia; Carnegie Mellon University / INESC-ID",
            "Isabel Trancoso; INESC-ID / IST",
            "Bhiksha Raj; Carnegie Mellon University"
        ],
        "Paper Title": "QUERYING DEPRESSION VLOGS",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Evaluation methodologies: Healthcare:"
    },
    {
        "Abstract": "A high performance automatic speech recognition (ASR) system is an important constituent component of an automatic language assessment system for free speaking language tests. The ASR system is required to be capable of recognising non-native spontaneous English speech and to be deployable under real-time conditions. The performance of ASR systems can often be significantly improved by leveraging upon multiple systems that are complementary, such as an ensemble. Ensemble methods, however, can be computationally expensive, often requiring multiple decoding runs, which makes them impractical for deployment. In this paper, a lattice-free implementation of sequence-level teacher-student training is used to reduce this computational cost, thereby allowing for real-time applications. This method allows a single student model to emulate the performance of an ensemble of teachers, but without the need for multiple decoding runs. Adaptations of the student model to speakers from different first languages (L1s) and grades are also explored.",
        "Authors": [
            "Yu Wang; Univerisity of Cambridge",
            "Jeremy Wong; Univerisity of Cambridge",
            "Mark Gales; Univerisity of Cambridge",
            "Kate Knill; Univerisity of Cambridge",
            "Anton Ragni; Univerisity of Cambridge"
        ],
        "Paper Title": "SEQUENCE TEACHER-STUDENT TRAINING OF ACOUSTIC MODELS FOR AUTOMATIC FREE SPEAKING LANGUAGE ASSESSMENT",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Corpora and Evaluation Methodologies",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Automatic identification of speakers' native language (L1) based on their speech in a second language (L2) is a challenging research problem that can aid several spoken language technologies such as automatic speech recognition (ASR), speaker recognition, and voice biometrics in interactive voice applications. End-to-end learning, in which the features and the classification model are learned jointly in a single system, is an emerging field in the areas of speech recognition, speaker verification and spoken language understanding. In this paper, we present our study on attention-based end-to-end modeling for native language identification on a database of 11 different L1s. Using this methodology, we can determine the native language of the speaker directly from the raw acoustic features. Experimental results from our study show that our best end-to-end model can achieve promising results by capturing speech commonalities across L1s using an attention mechanism. In addition, fusion of proposed systems with the baseline system leads to significant performance improvements.",
        "Authors": [
            "Rutuja Ubale; Educational Testing Service Research",
            "Yao Qian; Educational Testing Service Research",
            "Keelan Evanini; Educational Testing Service Research"
        ],
        "Paper Title": "EXPLORING END-TO-END ATTENTION-BASED NEURAL NETWORKS FOR NATIVE LANGUAGE IDENTIFICATION",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Playing recorded speech samples of an enrolled speaker – “replay attack” – is a simple approach to bypass an automatic speaker verification (ASV) system. The vulnerability of ASV systems to such attacks has been acknowledged and studied, but there has been no research into what spoofing detection systems are actually learning to discriminate. In this paper, we analyse the local behaviour of a replay spoofing detection system based on convolutional neural networks (CNN) adapted from a state-of-the-art CNN (LCNN_FFT) submitted at the ASVspoof 2017 challenge. We generate temporal and spectral explanations for predictions of the model using the SLIME algorithm. Our findings suggest that in most instances of spoofing the model is using information in the first 400 milliseconds of each audio instance to make the class prediction. Knowledge of the characteristics that spoofing detection systems are exploiting can help build less vulnerable ASV systems, other spoofing detection systems, as well as better evaluation databases.",
        "Authors": [
            "Bhusan Chettri; Queen Mary University of London",
            "Saumitra Mishra; Queen Mary University of London",
            "Bob L. Sturm; KTH Royal Institute of Engineering",
            "Emmanouil Benetos; Queen Mary University of London"
        ],
        "Paper Title": "Analysing the predictions of a CNN-based replay spoofing detection system",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Recent research on generative adversarial nets (GAN) for language identification (LID) has shown promising results. In this paper, we further exploit the latent abilities of GAN networks to firstly combine them with deep neural network (DNN)-based i-vector approaches and then to improve the LID model using conditional generative adversarial net (cGAN) classification. First, phoneme dependent deep bottleneck features (DBF) combined with output posteriors of a pre-trained DNN for automatic speech recognition (ASR) are used to extract i-vectors in the normal way. These i-vectors are then classified using cGAN, and we show an effective method within the cGAN to optimize parameters by combining both language identification and verification signals as supervision.Results show firstly that cGAN methods can significantly outperform DBF DNN i-vector methods where 49-dimensional i-vectors are used, but not where 600-dimensional vectors are used.Secondly, training a cGAN discriminator network for direct classification has further benefit for low dimensional i-vectors as well as short utterances with high dimensional i-vectors.However, incorporating a dedicated discriminator network output layer for classification and optimizing both classification and verification loss brings benefits in all test cases.",
        "Authors": [
            "Xiaoxiao Miao; The University of Kent / Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics / University of Chinese Academy of Sciences",
            "Ian McLoughlin; The University of Kent",
            "Shengyu Yao; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics / University of Chinese Academy of Sciences",
            "Yonghong Yan; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics / University of Chinese Academy of Sciences / Xinjiang Key Laboratory of Minority Speech and Language Information Processing, Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences"
        ],
        "Paper Title": "IMPROVED CONDITIONAL GENERATIVE ADVERSARIAL NET CLASSIFICATION FOR SPOKEN LANGUAGE RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "In this paper, we explore the use of a factorized hierarchical variational autoencoder (FHVAE) model to learn an unsupervised latent representation for dialect identification (DID). An FHVAE can learn a latent space that separates the more static attributes within an utterance from the more dynamic attributes by encoding them into two different sets of latent variables. Useful factors for dialect identification, such as phonetic or linguistic content, are encoded by a segmental latent variable, while irrelevant factors that are relatively constant within a sequence, such as a channel or speaker information, are encoded by sequential latent variable. The disentanglement property makes the segmental latent variable less susceptible to channel and speaker variation, and thus reduces degradation from channel domain mismatch. We demonstrate that on fully-supervised DID tasks, an end-to-end model trained on the features extracted from the FHVAE model achieves the best performance, compared to the same model trained on conventional acoustic features and an i-vector based system. Moreover, we show that the proposed approach can leverage a large amount of unlabeled data for FHVAE training to learn domain-invariant features for DID, and significantly improve the performance in low-resource condition, where the labels for the in-domain data are not available.",
        "Authors": [
            "Suwon Shon; Massachusetts Institute of Technology",
            "Wei-Ning Hsu; Massachusetts Institute of Technology",
            "James Glass; Massachusetts Institute of Technology"
        ],
        "Paper Title": "Unsupervised Representation Learning of Speech for Dialect Identification",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",
        "Authors": [
            "Seunghyun Yoon; Seoul National University",
            "Seokhyun Byun; Seoul National University",
            "Kyomin Jung; Seoul National University"
        ],
        "Paper Title": "MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Emotion recognition from speech:"
    },
    {
        "Abstract": "Computational paralinguistics is an area which contains diverse classification tasks. In many cases the class distribution of these tasks is highly imbalanced by nature, as the phenomena needed to detect in human speech do not occur uniformly. To ignore this imbalance, it is common to measure the efficiency of classification approaches via the Unweighted Average Recall (UAR) metric in this area. However, general classification methods such as Support-Vector Machines (SVM) and Deep Neural Networks (DNNs) were shown to focus on traditional classification accuracy, which might lead to a suboptimal performance for imbalanced datasets. In this study we show that by performing posterior calibration, this effect can be countered and the UAR scores obtained might be improved. Our approach led to relative error reduction values of 4% and 14% on the test set of two multi-class paralinguistic datasets that had imbalanced class distributions, outperforming the traditional downsampling.",
        "Authors": [
            "Gábor Gosztolya; MTA-SZTE Research Group on Artificial Intelligence",
            "Róbert Busa-Fekete; Yahoo Research Inc."
        ],
        "Paper Title": "POSTERIOR CALIBRATION FOR MULTI-CLASS PARALINGUISTIC CLASSIFICATION",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Emotion recognition from speech:"
    },
    {
        "Abstract": "In this work, we study the use of attention mechanisms to enhance the performance of the state-of-the-art deep learning model in Speech Emotion Recognition (SER). We introduce a new Long Short-Term Memory (LSTM)-based neural attention model which is able to take into account the temporal information in speech during the computation of the attention vector. The proposed LSTM-based model is evaluated on the IEMOCAP dataset using a 5-fold cross-validation scheme and achieved 68.8% weighted accuracy on 4 classes, which outperforms other state-of-the-art models.",
        "Authors": [
            "Gaetan Ramet; Ecole Polytechnique Federale de Lausanne",
            "Philip N. Garner; Idiap Research Institute",
            "Michael Baeriswyl; Swisscom",
            "Alexandros Lazaridis; Swisscom"
        ],
        "Paper Title": "CONTEXT-AWARE ATTENTION MECHANISM FOR SPEECH EMOTION RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Emotion recognition from speech:"
    },
    {
        "Abstract": "Automatic speaker verification (ASV) systems can be easily spoofed by previously recorded speech, synthesized speech and speech signal that artificially generated by voice conversion techniques. In order to increase the reliability of the ASV systems, detecting spoofing attacks whether a given speech signal is genuine or spoofed plays an important role. In this paper, we consider the detection of replay attacks which is the most easily implementable attack type against ASV systems. To this end, we utilize a deep neural network (DNN) based classifier using features extracted from the long-term average spectrum. The experiments are conducted on the latest edition of Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2017) database. The results obtained using the DNN classifier are compared with the ASVspoof 2017 baseline system provided by the organizers which consist of Gaussian mixture model (GMM) with constant-Q transform cepstral coefficients (CQCC) and the GMM with standard mel-frequency cepstrum coefficients (MFCC) features. Experimental results reveal that DNN considerably outperforms the well-known and successful GMM classifier. It is found that LTAS based features give better spoofing detection performance than CQCC and MFCC. Finally, we find that high-frequency region components convey much more discriminative information independent of features and classifiers.",
        "Authors": [
            "Bekir Bakar; Bursa Technical University",
            "Cemal Hanilci; Bursa Technical University"
        ],
        "Paper Title": "AN EXPERIMENTAL STUDY ON AUDIO REPLAY ATTACK DETECTION USING DEEP NEURAL NETWORKS",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This article presents a whisper speech detector in the far-field domain. The proposed system consists of a long-short term memory (LSTM) neural network trained on log-filterbank energy (LFBE) acoustic features. This model is trained and evaluated on recordings of human interactions with voice- controlled, far-field devices in whisper and normal phonation modes. We compare multiple inference approaches for utterance-level classification by examining trajectories of the LSTM posteriors. In addition, we engineer a set of features based on the signal characteristics inherent to whisper speech, and evaluate their effectiveness in further separating whisper from normal speech. A benchmarking of these features using multilayer perceptrons (MLP) and LSTMs suggests that the proposed features, in combination with LFBE features, can help us further improve our classifiers. We prove that, with enough data, the LSTM model is indeed as capable of learn- ing whisper characteristics from LFBE features alone com- pared to a simpler MLP model that uses both LFBE and features engineered for separating whisper and normal speech. In addition, we prove that the LSTM classifiers accuracy can be further improved with the incorporation of the proposed engineered features.",
        "Authors": [
            "Zeynab Raeesy; Amazon",
            "Kellen Gillespie; Amazon",
            "Chengyuan Ma; Amazon",
            "Thomas Drugman; Amazon",
            "Jiacheng Gu; Amazon",
            "Roland Maas; Amazon",
            "Ariya Rastrow; Amazon",
            "Björn Hoffmeister; Amazon"
        ],
        "Paper Title": "LSTM-BASED WHISPER DETECTION",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We address the problem of American Sign Language fingerspelling recognition ``in the wild'', using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and connectionist temporal classification-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.",
        "Authors": [
            "Bowen Shi; Toyota Technological Institute at Chicago",
            "Aurora Martinez Del Rio; University of Chicago",
            "Jonathan Keane; University of Chicago",
            "Jonathan Michaux; Toyota Technological Institute at Chicago",
            "Diane Brentari; University of Chicago",
            "Greg Shakhnarovich; Toyota Technological Institute at Chicago",
            "Karen Livescu; Toyota Technological Institute at Chicago"
        ],
        "Paper Title": "AMERICAN SIGN LANGUAGE FINGERSPELLING RECOGNITION IN THE WILD",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    },
    {
        "Abstract": "This paper presents a WaveNet-based zero-delay lossless speech coding technique for high-quality communications. The WaveNet generative model, which is a state-of-the-art model for neural-network-based speech waveform synthesis, is used in both the encoder and decoder. In the encoder, discrete speech signals are losslessly compressed using sample-by-sample entropy coding. The decoder fully reconstructs the original speech signals from the compressed signals without algorithmic delay. Experimental results show that the proposed coding technique can transmit speech audio waveforms with 50% their original bit rate and the WaveNet-based speech coder remains effective for unknown speakers.",
        "Authors": [
            "Takenori Yoshimura; Nagoya Institute of Technology",
            "Kei Hashimoto; Nagoya Institute of Technology",
            "Keiichiro Oura; Nagoya Institute of Technology",
            "Yoshihiko Nankaku; Nagoya Institute of Technology",
            "Keiichi Tokuda; Nagoya Institute of Technology"
        ],
        "Paper Title": "WAVENET-BASED ZERO-DELAY LOSSLESS SPEECH CODING",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We address the problem of reconstructing articulatory movements, given audio and/or phonetic labels. The scarce availability of multi-speaker articulatory data makes it difficult to learn a reconstruction that generalizes to new speakers and across datasets. We first consider the XRMB dataset where audio, articulatory measurements and phonetic transcriptions are available. We show that phonetic labels, used as input to deep recurrent neural networks that reconstruct articulatory features, are in general more helpful than acoustic features in both matched and mismatched training-testing conditions. In a second experiment, we test a novel approach that attempts to build articulatory features from prior articulatory information extracted from phonetic labels. Such approach recovers vocal tract movements directly from an acoustic-only dataset without using any articulatory measurement. Results show that articulatory features generated by this approach can correlate up to 0.59 Pearson’s product-moment correlation with measured articulatory features.",
        "Authors": [
            "Rosanna Turrisi; Istituto Italiano di Tecnologia",
            "Raffaele Tavarone; Istituto Italiano di Tecnologia",
            "Leonardo Badino; Istituto Italiano di Tecnologia"
        ],
        "Paper Title": "IMPROVING GENERALIZATION OF VOCAL TRACT FEATURE RECONSTRUCTION: FROM AUGMENTED ACOUSTIC INVERSION TO ARTICULATORY FEATURE RECONSTRUCTION WITHOUT ARTICULATORY DATA",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    },
    {
        "Abstract": "In this paper we present a data driven vocal tract area function (VTAF) estimation using Deep Neural Networks (DNN). We approach the VTAF estimation problem based on sequence to sequence learning neural networks, where regression over a sliding window is used to learn arbitrary non-linear one-to-many mapping from the input feature sequence to the target articulatory sequence. We propose two schemes for efficient estimation of the VTAF; (1) a direct estimation of the area function values and (2) an indirect estimation via predicting the vocal tract boundaries. We consider acoustic speech and phone sequence as two possible input modalities for the DNN estimators. Experimental evaluations are performed over a large data comprising acoustic and phonetic features with parallel articulatory information from the USC-TIMIT database. Our results show that the proposed direct and indirect schemes perform the VTAF estimation with mean absolute error (MAE) rates lower than 1.65~mm, where the direct estimation scheme is observed to perform better than the indirect scheme.",
        "Authors": [
            "Sasan Asadiabadi; Koc university",
            "Engin Erzin; Koc university"
        ],
        "Paper Title": "A DEEP LEARNING APPROACH FOR DATA DRIVEN VOCAL TRACT AREA FUNCTION ESTIMATION",
        "Presentation": "Poster",
        "Presentation #": "13",
        "Presentation Time": "Wednesday, December 19, 13:30 - 15:30",
        "Session": "Detection, Paralinguistics and Coding",
        "Session Time": "Wednesday, December 19, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recent advances in spoken language technologies and the introduction of many customer facing products, have given rise to a wide customer reliance on smart personal assistants for many of their daily tasks. In this paper, we present a system to reduce users' cognitive load by extending personal assistants with long-term personal memory where users can store and retrieve by voice, arbitrary pieces of information. The problem is framed as a neural retrieval based question answering system where answers are selected from previously stored user memories. We propose to directly optimize the end-to-end retrieval performance, measured by the F1-score, using reinforcement learning, leading to better performance on our experimental test set(s).",
        "Authors": [
            "Rasool Fakoor; Amazon",
            "Amanjit Kainth; Amazon",
            "Siamak Shakeri; Amazon",
            "Christopher Winestock; Amazon",
            "Abdel-rahman Mohamed; Amazon",
            "Ruhi Sarikaya; Amazon"
        ],
        "Paper Title": "DIRECT OPTIMIZATION OF F-MEASURE FOR RETRIEVAL-BASED PERSONAL QUESTION ANSWERING",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken document retrieval:"
    },
    {
        "Abstract": "This paper is concerned with the training of recurrent neural networks as goal-oriented dialog agents using reinforcement learning. Training such agents with policy gradients typically requires a large amount of samples. However, the collection of the required data in form of conversations between chat-bots and human agents is time-consuming and expensive. To mitigate this problem, we describe an efficient policy gradient method using positive memory retention, which significantly increases the sample-efficiency. We show that our method is 10 times more sample-efficient than policy gradients in extensive experiments on a new synthetic number guessing game. Moreover, in a real-word visual object discovery game, the proposed method is twice as sample-efficient as policy gradients and shows state-of-the-art performance.",
        "Authors": [
            "Rui Zhao; Siemens & LMU",
            "Volker Tresp; Siemens & LMU"
        ],
        "Paper Title": "EFFICIENT DIALOG POLICY LEARNING VIA POSITIVE MEMORY RETENTION",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "Going beyond turn-taking models built to solve specific tasks, such as predicting if a user will hold his/her turn after a pause, there is growing interest in more general models for turn taking that subsume many such tasks, and good results have recently been obtained (Skantze 2017). Here we present a recurrent network model that outperforms (Skanze 2017) and does so without requiring lexical annotation. Further, we show that this model can be trained for different languages with no modifications, providing good results in turn-taking prediction for English, Spanish, Japanese, Mandarin and French. We also show that our model performs well across genres, including task-oriented dialog and general conversation.",
        "Authors": [
            "Nigel Ward; University of Texas at El Paso",
            "Diego Aguirre; University of Texas at El Paso",
            "Gerardo Cervantes; University of Texas at El Paso",
            "Olac Fuentes; University of Texas at El Paso"
        ],
        "Paper Title": "TURN-TAKING PREDICTIONS ACROSS LANGUAGES AND GENRES USING AN LSTM RECURRENT NEURAL NETWORK",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "In this paper we investigate the novel use of exclusively audio to predict whether a spoken dialogue will be successful or not, both in a subjective and in an objective manner. To achieve that, multiple spectral and rhythmic features are inputted to support vector machines and deep neural networks. We report results on data from 3267 spoken dialogues, using both the full user response as well as parts of it. Experiments show an average accuracy of 74% can be achieved using just 5 acoustic features, when analysing merely 1 user turn, which allows both a real-time but also a fairly accurate prediction of a dialogue successfulness only after one short interaction unit. From the features tested, those related to speech rate, signal energy and cepstrum are amongst the most informative. Results presented here outperform the state of the art in spoken dialogue success prediction through solely acoustic features.",
        "Authors": [
            "Athanasios Lykartsis; Technische Universität Berlin",
            "Margarita Kotti; Toshiba LTD",
            "Alexandros Papangelis; Toshiba LTD",
            "Yannis Stylianou; Toshiba LTD"
        ],
        "Paper Title": "PREDICTION OF DIALOGUE SUCCESS WITH SPECTRAL AND RHYTHM ACOUSTIC FEATURES USING DNNS AND SVMS",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "Dialog response ranking is used to rank response candidates by considering their relation to the dialog history. Although researchers have addressed this concept for open-domain dialogs, little attention has been focused on task-oriented dialogs. Furthermore, no previous studies have analyzed whether response ranking can improve the performance of existing dialog systems in real human--computer dialogs with speech recognition errors. In this paper, we propose a context-aware dialog response re-ranking system. Our system reranks responses in two steps: (1) it calculates matching scores for each candidate response and the current dialog context; (2) it combines the matching scores and a probability distribution of the candidates from an existing dialog system for response re-ranking. By using neural word embedding-based models and handcrafted or logistic regression-based ensemble models, we have improved the performance of a recently proposed end-to-end task-oriented dialog system on real dialogs with speech recognition errors.",
        "Authors": [
            "Junki Ohmura; Sony Corporation",
            "Maxine Eskenazi; Language Technologies Institute Carnegie Mellon University"
        ],
        "Paper Title": "CONTEXT-AWARE DIALOG RE-RANKING FOR TASK-ORIENTED DIALOG SYSTEMS",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "This paper proposes an approach to detecting out-of-domain slot values from user utterances in spoken dialogue systems based on contexts. The approach detects keywords of slot values from utterances and consults domain knowledge (i.e., an ontology) to check whether the keywords are out-of-domain. This can prevent the systems from responding improperly to user requests. We use a Recurrent Neural Network (RNN) encoder-decoder model and propose a method that uses only in-domain data. The method replaces word embedding vectors of the keywords corresponding to slot values with random vectors during training of the model. This allows using context information. The model is robust against over-fitting problems because it is independent of the slot values of the training data. Experiments show that the proposed method achieves a 65% gain in F1 score relative to a baseline model and a further 13 percentage points by combining with other methods.",
        "Authors": [
            "Yuka Kobayashi; Toshiba Corporation",
            "Takami Yoshida; Toshiba Corporation",
            "Kenji Iwata; Toshiba Corporation",
            "Hiroshi Fujimura; Toshiba Corporation",
            "Masami Akamine; Toshiba Corporation"
        ],
        "Paper Title": "OUT-OF-DOMAIN SLOT VALUE DETECTION FOR SPOKEN DIALOGUE SYSTEMS WITH CONTEXT INFORMATION",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "While neural conversational models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, training neural models from scratch requires an enormous amount of data. If pre-trained models can be reused when they have many things in common with a new task, we can significantly cut down the amount of required data. To achieve this goal, we adopt a neural continual learning algorithm to allow a conversational agent to accumulate skills across different tasks in a data-efficient way. We present preliminary results on conversational skill accumulation on multiple task-oriented domains.",
        "Authors": [
            "Sungjin Lee; Microsoft Research"
        ],
        "Paper Title": "Accumulating Conversational Skills using Continual Learning",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "Learning goal-oriented dialogues by means of deep reinforcement learning has recently become a popular research topic. However, commonly used policy-based dialogue agents often end up focusing on simple utterances and suboptimal policies. To mitigate this problem, we propose a class of novel temperature-based extensions for policy gradient methods, which are referred to as Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the GuessWhat?! game, we achieve significant improvements with two innovations. The first one is an extension of the state-of-the-art solutions with Seq2Seq and Memory Network structures that leads to an improvement of 7%. The second one is the application of our newly developed TPG methods, which improves the performance additionally by around 5% and, even more importantly, helps produce more convincing utterances.",
        "Authors": [
            "Rui Zhao; Siemens & LMU",
            "Volker Tresp; Siemens & LMU"
        ],
        "Paper Title": "LEARNING GOAL-ORIENTED VISUAL DIALOG VIA TEMPERED POLICY GRADIENT",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "The potential of dialogue systems to address user's emotional need has steadily grown. In particular, we focus on dialogue systems application to promote positive emotional states, an attempt to imitate emotional support between humans. Positive emotion elicitation takes form as chat-based dialogue interactions that is layered with an implicit goal to improve user's emotional state. To this date, existing approaches have only relied on mimicking the target responses without considering their emotional impact, i.e., the change of emotional state they cause on the listener, in the model itself. In this paper, we propose explicitly utilizing emotional impact information to optimize neural dialogue system towards generating responses that elicit positive emotion. We examine two emotion-rich corpora collected in different scenarios: Wizard-of-Oz and spontaneous. Evaluation shows that the proposed method yields lower perplexity, as well as produces responses that are perceived as more natural and likely to elicit a more positive emotion.",
        "Authors": [
            "Nurul Lubis; Nara Institute of Science and Technology",
            "Sakriani Sakti; Nara Institute of Science and Technology",
            "Koichiro Yoshino; Nara Institute of Science and Technology",
            "Satoshi Nakamura; Nara Institute of Science and Technology"
        ],
        "Paper Title": "Optimizing neural response generator with emotional impact information",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "A crucial step in task-oriented dialogue systems is tracking the user's goal over the course of the conversation. This involves maintaining a probability distribution over possible values for each slot (e.g., the food slot might map to the value Turkish), which gets updated at each turn of the dialogue. Previously, rule-based methods were applied to dialogue systems, or models that required hand-crafted semantic dictionaries mapping phrases to those that are similar in meaning (e.g., area might map to part of town). However, these are expensive to design for each domain, limiting the generalizability. In addition, often a spoken language understanding (SLU) component precedes the dialogue state update mechanism; however, this leads to compounded errors as the output from one module is passed to the next. Instead, more recent work has explored deep learning models for directly updating dialogue state, bypassing the need for SLU or expert-engineered rules. We demonstrate that a novel convolutional neural architecture without any pre-trained word vectors or semantic dictionaries achieves 86.9% joint goal accuracy and 95.4% requested slot accuracy on WOZ 2.0.",
        "Authors": [
            "Mandy Korpusik; Massachusetts Institute of Technology",
            "James Glass; Massachusetts Institute of Technology"
        ],
        "Paper Title": "CONVOLUTIONAL NEURAL NETWORKS FOR DIALOGUE STATE TRACKING WITHOUT PRE-TRAINED WORD VECTORS OR SEMANTIC DICTIONARIES",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "Accurate prediction of conversation topics can be a valuable signal for creating coherent and engaging dialog systems. In this work, we focus on context-aware topic classification methods for identifying topics in free-form human-chatbot dialogs. We extend previous work on neural topic classification and unsupervised topic keyword detection by incorporating conversational context and dialog act features. On annotated data, we show that incorporating context and dialog acts leads to relative gains in topic classification accuracy by 35% and on unsupervised keyword detection recall by 11% for conversational interactions where topics frequently span multiple utterances. We show that topical metrics such as topical depth is highly correlated with dialog evaluation metrics such as coherence and engagement implying that conversational topic models can predict user satisfaction. Our work for detecting conversation topics and keywords can be used to guide chatbots towards coherent dialog.",
        "Authors": [
            "Chandra Khatri; Amazon Alexa",
            "Rahul Goel; Amazon Alexa",
            "Behnam Hedayatnia; Amazon Alexa",
            "Angeliki Metanillou; Amazon Alexa",
            "Anushree Venkatesh; Amazon Alexa",
            "Raefer Gabriel; Amazon Alexa",
            "Arindam Mandal; Amazon Alexa"
        ],
        "Paper Title": "Contextual Topic Modeling for Dialog Systems",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "In this paper, we introduce end-to-end neural network based models for simulating users in task-oriented dialogue systems. User simulation in dialogue systems is crucial from two different perspectives: (i) automatic evaluation of different dialogue models, and (ii) training individual dialogue system components. We design a hierarchical sequence-tosequence model that first encodes the initial user goal and system turns into fixed length representations using Recurrent Neural Networks (RNN). It then encodes the dialogue history using another RNN layer. At each turn, user responses are decoded from the hidden representations of the dialogue level RNN. This hierarchical user simulator (HUS) approach allows the model to capture undiscovered parts of the user goal without the need of an explicit dialogue state tracking. We further develop several variants by utilizing a latent variable model to inject random variations into user responses to promote diversity and a novel goal regularization mechanism to penalize divergence of user responses from the initial user goal. We evaluate the proposed models on a movie ticket booking domain by systematically interacting each user simulator with various dialogue system policies trained with different objectives and users.",
        "Authors": [
            "Izzeddin Gur; University of California Santa Barbara",
            "Dilek Hakkani-Tür; Google",
            "Gokhan Tür; Uber AI Labs",
            "Pararth Shah; Facebook"
        ],
        "Paper Title": "USER MODELING FOR TASK ORIENTED DIALOGUES",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Spoken dialog systems:"
    },
    {
        "Abstract": "Language models (LM) for interactive speech recognition systems are trained on large amounts of data and the model parameters are optimized on past user data. New application intents and interaction types are released for these systems over time, imposing challenges to adapt the LMs since the existing training data is no longer sufficient to model the future user interactions. It is unclear how to adapt LMs to new application intents without degrading the performance on existing applications. In this paper, we propose a solution to (a) estimate n-gram counts directly from the hand-written grammar for training LMs and (b) use constrained optimization to optimize the system parameters for future use cases, while not degrading the performance on past usage. We evaluated our approach on new applications intents for a personal assistant system and find that the adaptation improves the word error rate by up to 15% on new applications even when there is no adaptation data available for an application.",
        "Authors": [
            "Ankur Gandhe; Amazon",
            "Ariya Rastrow; Amazon",
            "Björn Hoffmeister; Amazon"
        ],
        "Paper Title": "SCALABLE LANGUAGE MODEL ADAPTATION FOR SPOKEN DIALOGUE SYSTEMS",
        "Presentation": "Poster",
        "Presentation #": "13",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Dialogue",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper we present a new method for text-independent speaker verification that combines segmental dynamic time warping (SDTW) and the d-vector approach. The d-vectors, generated from a feed forward deep neural network trained to distinguish between speakers, are used as features to perform alignment and hence calculate the overall distance between the enrolment and test utterances.We present results on the NIST 2008 data set for speaker verification where the proposed method outperforms the conventional i-vector baseline with PLDA scores and outperforms d-vector approach with local distances based on cosine and PLDA scores. Also score combination with the i-vector/PLDA baseline leads to significant gains over both methods.",
        "Authors": [
            "Mohamed Adel; Microsoft Advanced Technology Lab, Cairo",
            "Mohamed Afify; Microsoft Advanced Technology Lab, Cairo",
            "Akram Gaballah; Microsoft Corporation",
            "Magda Fayek; Cairo University"
        ],
        "Paper Title": "Text-Independent Speaker Verification Based on Deep Neural Networks and Segmental Dynamic Time Warping",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "In this paper, we propose a Convolutional Neural Network (CNN) based speaker recognition model for extracting robust speaker embeddings. The embedding can be extracted efficiently with linear activation in the embedding layer. To understand how the speaker recognition model operates with text-independent input, we modify the structure to extract frame-level speaker embeddings from each hidden layer. We feed utterances from the TIMIT dataset to the trained network and use several proxy tasks to study the networks ability to represent speech input and differentiate voice identity. We found that the networks are better at discriminating broad phonetic classes than individual phonemes. In particular, frame-level embeddings that belong to the same phonetic classes are similar (based on cosine distance) for the same speaker. The frame level representation also allows us to analyze the networks at the frame level, and has the potential for other analyses to improve speaker recognition.",
        "Authors": [
            "Suwon Shon; Massachusetts Institute of Technology",
            "Hao Tang; Massachusetts Institute of Technology",
            "James Glass; Massachusetts Institute of Technology"
        ],
        "Paper Title": "FRAME-LEVEL SPEAKER EMBEDDINGS FOR TEXT-INDEPENDENT SPEAKER RECOGNITION AND ANALYSIS OF END-TO-END MODEL",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Speaker verification (SV) systems using deep neural network embeddings, so-called the x-vector systems, are becoming popular due to its good performance superior to the i-vector systems. The fusion of these systems provides improved performance benefiting both from the discriminatively trained x-vectors and generative i-vectors capturing distinct speaker characteristics. In this paper, we propose a novel method to include the complementary information of i-vector and x-vector, that is called generative x-vector. The generative x-vector utilizes a transformation model learned from the i-vector and x-vector representations of the background data. Canonical correlation analysis is applied to derive this transformation model, which is later used to transform the standard x-vectors of the enrollment and test segments to the corresponding generative x-vectors. The SV experiments performed on the NIST SRE 2010 dataset demonstrate that the system using generative x-vectors provides considerably better performance than the baseline i-vector and x-vector systems. Furthermore, the generative x-vectors outperform the fusion of i-vector and x-vector systems for long-duration utterances, while yielding comparable results for short-duration utterances.",
        "Authors": [
            "Longting Xu; National University of Singapore",
            "Rohan Kumar Das; National University of Singapore",
            "Emre Yilmaz; National University of Singapore",
            "Jichen Yang; National University of Singapore",
            "Haizhou Li; National University of Singapore"
        ],
        "Paper Title": "GENERATIVE X-VECTORS FOR TEXT-INDEPENDENT SPEAKER VERIFICATION",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Deep learning is progressively gaining popularity as a viable alternative to i-vectors for speaker recognition. Promising results have been recently obtained with Convolutional Neural Networks (CNNs) when fed by raw speech samples directly. Rather than employing standard hand-crafted features, the latter CNNs learn low-level speech representations from waveforms, potentially allowing the network to better capture important narrow-band speaker characteristics such as pitch and formants. Proper design of the neural network is crucial to achieve this goal. This paper proposes a novel CNN architecture, called SincNet, that encourages the first convolutional layer to discover more meaningful filters. SincNet is based on parametrized sinc functions, which implement band-pass filters. In contrast to standard CNNs, that learn all elements of each filter, only low and high cutoff frequencies are directly learned from data with the proposed method. This offers a very compact and efficient way to derive a customized filter bank specifically tuned for the desired application. Our experiments, conducted on both speaker identification and speaker verification tasks, show that the proposed architecture converges faster and performs better than a standard CNN on raw waveforms.",
        "Authors": [
            "Mirco Ravanelli; Université de Montréal",
            "Yoshua Bengio; Université de Montréal"
        ],
        "Paper Title": "SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "We investigate the use of deep neural networks (DNNs) for the speaker diarization task to improve performance under domain mismatched conditions. Three unsupervised domain adaptation techniques, namely inter-dataset variability compensation (IDVC), domain-invariant covariance normalization (DICN), and domain mismatch modeling (DMM), are applied on DNN based speaker embeddings to compensate for the mismatch in the embedding subspace. We present results conducted on the DIHARD data, which was released for the 2018 diarization challenge. Collected from a diverse set of domains, this data provides very challenging domain mismatched conditions for the diarization task. Our results provide insights on how the performance of our proposed system could be further improved.",
        "Authors": [
            "Ivan Himawan; Queensland University of Technology",
            "Md Hafizur Rahman; Queensland University of Technology",
            "Sridha Sridharan; Queensland University of Technology",
            "Clinton Fookes; Queensland University of Technology",
            "Ahilan Kanagasundaram; Queensland University of Technology"
        ],
        "Paper Title": "INVESTIGATING DEEP NEURAL NETWORKS FOR SPEAKER DIARIZATION IN THE DIHARD CHALLENGE",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Speaker Role Recognition (SRR) assigns a specific speaker role to each speaker-homogeneous speech segment in a conversation. Typically, those segments have to be identified first through a diarization step. Additionally, since SRR is usually based on the different linguistic patterns observed between the roles to be recognized, an Automatic Speech Recognition (ASR) system is also indispensable for the task in hand to convert speech to text. In this work we introduce a Role Annotated Speech Recognition (RASR) system which, given a speech signal, outputs a sequence of words annotated with the corresponding speaker roles. Thus, the need of different component modules which are connected in a way that may lead to error propagation is eliminated. We present, analyze, and test our system for the case of two speaker roles to showcase an end-to-end approach for automatic rich transcription with application to clinical dyadic interactions.",
        "Authors": [
            "Nikolaos Flemotomos; University of Southern California",
            "Zhuohao Chen; University of Southern California",
            "David Atkins; University of Washington",
            "Shrikanth Narayanan; University of Southern California"
        ],
        "Paper Title": "ROLE ANNOTATED SPEECH RECOGNITION FOR CONVERSATIONAL INTERACTIONS",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "This paper investigates text-independent speaker recognitionusing neural embedding extractors based on the time-delayneural network. Our primary focus is to explore the teacher-student (TS) training framework for knowledge distillation ina text-independent (TI) speaker recognition task. We reportthe results on both proprietary and public benchmarks, ob-taining competitive results with 88-93% smaller models. Par-ticularly, in the clean testing conditions, we find TS trainingon neural-based TI systems achieved the same or better per-formance than the i-vector based counterparts. Neural embed-dings are less prone to short segment issues, and offer betterperformance particularly in the high recall setting. They canalso provide some additional insights about speakers, such asgender or how difficult a given speaker can be for recognition.",
        "Authors": [
            "Raymond W. M. Ng; Emotech Labs",
            "Xuechen Liu; Emotech Labs",
            "Pawel Swietojanski; The University of New South Wales"
        ],
        "Paper Title": "TEACHER-STUDENT TRAINING FOR TEXT-INDEPENDENT SPEAKER RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "This paper presents an experimental study on deep speaker embedding with an attention mechanism that has been found to be a powerful representation learning technique in speaker recognition. In this framework, an attention model works as a frame selector that computes an attention weight for each frame-level feature vector, in accord with which an utterance-level representation is produced at the pooling layer in a speaker embedding network. In general, an attention model is trained together with the speaker embedding network on a single objective function, and thus those two components are tightly bound to one another. In this paper, we consider the possibility that the attention model might be decoupled from its parent network and assist other speaker embedding networks and even conventional i-vector extractors. This possibility is demonstrated through a series of experiments on a NIST Speaker Recognition Evaluation (SRE) task, with 9.0% EER reduction and 3.8% minC_primary reduction when the attention weights are applied to i-vector extraction. Another experiment shows that DNN-based soft voice activity detection (VAD) can be effectively combined with the attention mechanism to yield further reduction of minC_primary by 6.6% and 1.6% in deep speaker embedding and i-vector systems, respectively.",
        "Authors": [
            "Qiongqiong Wang; NEC Corporation",
            "Koji Okabe; NEC Corporation",
            "Kong Aik Lee; NEC Corporation",
            "Hitoshi Yamamoto; NEC Corporation",
            "Takafumi Koshinaka; NEC Corporation"
        ],
        "Paper Title": "ATTENTION MECHANISM IN SPEAKER RECOGNITION: WHAT DOES IT LEARN IN DEEP SPEAKER EMBEDDING?",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Whisper is a commonly encountered form of speech that differs significantly from modal speech. As speaker recognition technology becomes more ubiquitous, it is important to assess the abilities and limitations of systems in the presence of variability such as whisper. In this paper, a comparative evaluation of whispered speaker recognition performance across two independent datasets is presented. Whisper-neutral speech comparisons are observed to consistently degrade performance relative to both neutral-neutral and whisper-whisper comparisons. An i-vector-based approach to whisper detection is introduced, and is shown to perform accurately across datasets even at short durations. The output of the whisper detector is subsequently used to select score calibration parameters for whispered speech comparisons, leading to a reduction in global calibration and discrimination error.",
        "Authors": [
            "Finnian Kelly; The University of Texas at Dallas",
            "John H.L. Hansen; The University of Texas at Dallas"
        ],
        "Paper Title": "Detection and calibration of whisper for speaker recognition",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "In this paper, we investigate training speaker recognition models using coarse-grained speaker labels provided only at the recording level. The approach is based on the recently proposed weakly supervised training method that allows to train a speaker recognition deep neural network using a special cost function that doesn't need segment-level annotations. Experiments are conducted on the VoxCeleb corpus. We show that without using any reference segment-level labeling, the method can achieve 1% speaker recognition error rate on the official VoxCeleb closed set speaker recognition test set, as opposed to 5.4% that was previously reported. By training a x-vector based speaker verification system on the resegmented and relabeled VoxCeleb corpus, we can achieve 4.57% EER on the VoxCeleb speaker verification test set which is a 17% relative improvement over the best system that uses the official VoxCeleb speaker annotations.",
        "Authors": [
            "Tanel Alumäe; Tallinn University of Technology"
        ],
        "Paper Title": "TRAINING SPEAKER RECOGNITION MODELS WITH RECORDING-LEVEL LABELS",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Short utterances cause performance degradation in conventional speaker recognition systems based on i-vector, which relies on the statistics of spectral features. To overcome this difficulty, we propose a novel method that utilizes the dynamics of the spectral features as well as their distribution. Our model integrates echo state network (ESN), a type of reservoir computing architecture, and self-organizing map (SOM), a competitive learning network. The ESN consists of a single-hidden-layer recurrent neural network with randomly fixed weights, which extracts temporal patterns of the spectral features. The input weights of our model are trained using the unsupervised competitive learning algorithm of the SOM, before enrollment, to extract the intrinsic structure of the spectral features, whereas the input weights are fixed randomly in the original ESN. In enrollment, the output weights are trained in a supervised manner to recognize an individual in a group of speakers. Our experiment demonstrates that the proposed method outperforms or is comparable to a baseline i-vector system for text-independent speaker identification on short utterances.",
        "Authors": [
            "Narumitsu Ikeda; The University of Tokyo",
            "Yoshinao Sato; Fairy Devices Inc.",
            "Hirokazu Takahashi; The University of Tokyo"
        ],
        "Paper Title": "SHORT UTTERANCE SPEAKER RECOGNITION BY RESERVOIR WITH SELF-ORGANIZED MAPPING",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Thursday, December 20, 10:00 - 12:00",
        "Session": "Speaker Recognition/Verification",
        "Session Time": "Thursday, December 20, 10:00 - 12:00",
        "Topic": "Speaker/language recognition:"
    },
    {
        "Abstract": "Neural abstractive summarization has been increasingly studied, where the prior work mainly focused on summarizing single-speaker documents (news, scientific publications, etc). In dialogues, there are diverse interactive patterns between speakers, which are usually defined as dialogue acts. The interactive signals may provide informative cues for better summarizing dialogues. This paper proposes to explicitly leverage dialogue acts in a neural summarization model, where a sentence-gated mechanism is designed for modeling the relationships between dialogue acts and the summary. The experiments show that our proposed model significantly improves the abstractive summarization performance compared to the state-of-the-art baselines on the AMI meeting corpus, demonstrating the usefulness of the interactive signal provided by dialogue acts.",
        "Authors": [
            "Chih-Wen Goo; National Taiwan University",
            "Yun-Nung Chen; National Taiwan University"
        ],
        "Paper Title": "ABSTRACTIVE DIALOGUE SUMMARIZATION WITH SENTENCE-GATED MODELING OPTIMIZED BY DIALOGUE ACTS",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Spoken document summarization:"
    },
    {
        "Abstract": "A novel graph-to-tree conversion mechanism called the deep-tree generation (DTG) algorithm is first proposed to predict text data represented by graphs. The DTG method can generate a richer and more accurate representation for nodes (or vertices) in graphs. It adds flexibility in exploring the vertex neighborhood information to better reflect the second order proximity and homophily equivalence in a graph. Then, a Deep-Tree Recursive Neural Network (DTRNN) method is presented and used to classify vertices that contains text data in graphs. To demonstrate the effectiveness of the DTRNN method, we apply it to three real-world graph datasets and show that the DTRNN method outperforms several state-of-the-art benchmarking methods.",
        "Authors": [
            "Fenxiao Chen; University of Southern California",
            "Bin Wang; University of Southern California",
            "C.-C. Jay Kuo; University of Southern California"
        ],
        "Paper Title": "Graph-based Deep-Tree Recursive Neural Network (DTRNN) for Text Classification",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "The objective of this work is to develop effective multi-view semi-supervised machine learning strategies for sentence boundary classification problem when only small sets of sentence boundary labeled data are available. We propose three-view and committee-based learning strategies incorporating with co-training algorithms with agreement, disagreement, and self-combined learning strategies using prosodic, lexical and morphological information. We compare experimental results of proposed three-view and committee-based learning strategies to other semi-supervised learning strategies in the literature namely, self-training and co-training with agreement, disagreement, and self-combined strategies. The experiment results show that sentence segmentation performance can be highly improved using multi-view learning strategies that we propose since data sets can be represented by three redundantly sufficient and disjoint feature sets. We show that the proposed strategies substantially improve the average performance when only a small set of manually labeled data is available for Turkish and English spoken languages, respectively.",
        "Authors": [
            "Dogan Dalva; F.M.V. ISIK University",
            "Umit Guz; F.M.V. ISIK University",
            "Hakan Gurkan; Bursa Technical University"
        ],
        "Paper Title": "EXTENSION OF CONVENTIONAL CO-TRAINING LEARNING STRATEGIES TO THREE-VIEW AND COMMITTEE-BASED LEARNING STRATEGIES FOR EFFECTIVE AUTOMATIC SENTENCE SEGMENTATION",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Neural cache language models (LMs) extend the idea of regular cache language models by making the cache probability dependent on the similarity between the current context and the context of the words in the cache. We make an extensive comparison of ‘regular’ cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results. Furthermore, we propose two extensions to this neural cache model that make use of the content value/information weight of the word: firstly, combining the cache probability and LM probability with an information-weighted interpolation and secondly, selectively adding only content words to the cache. We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs. Additionally, we observe significant WER reductions with respect to the baseline model on the WSJ ASR task.",
        "Authors": [
            "Lyan Verwimp; KU Leuven",
            "Joris Pelemans; Apple",
            "Hugo Van hamme; KU Leuven",
            "Patrick Wambacq; KU Leuven"
        ],
        "Paper Title": "INFORMATION-WEIGHTED NEURAL CACHE LANGUAGE MODELS FOR ASR",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Word segmentation from phoneme sequences is essential to identify unknown words (out-of-vocabulary; OOV) in spoken dialogues. The Pitman-Yor semi-Markov model (PYSMM) is used for word segmentation that handles dynamic increase in vocabularies. The obtained vocabularies, however, still include meaningless entries due to insufficient cues for phoneme sequences. We focus here on using subword information to capture patterns as ``words.'' We propose 1) a model based on subword $N$-gram and subword estimation using a vocabulary set, and 2) posterior fusion of the results of a PYSMM and our model to take advantage of both. Our experiments showed 1) the potential of using subword information for OOV acquisition, and 2) that our method outperformed the PYSMM by 1.53 and 1.07 in terms of the F-measure of the obtained OOV set for English and Japanese corpora, respectively.",
        "Authors": [
            "Ryu Takeda; Osaka University",
            "Kazunori Komatani; Osaka University",
            "Alexander Rudnicky; Carnegie Mellon University"
        ],
        "Paper Title": "WORD SEGMENTATION FROM PHONEME SEQUENCES BASED ON PITMAN-YOR SEMI-MARKOV MODEL EXPLOITING SUBWORD INFORMATION",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Traditional automatic question generation often requires hand-crafted templates or sophisticated NLP pipelines. Such approaches, however, require extensive labor and expertise to morphologically analyze the sentences and create the NLP framework. Our works aim to simplify these labors. We conduct a contrastive experiment between two types of sequence learning: statistical-based machine translation and attention-based sequence neural network. These models can be trained end-to-end, and it can capture the pattern between the input sequence and output sequence, thus diminishing the need to prepare a sophisticated NLP pipeline. Automatic evaluation results show that our system outperforms the state-of-the-art rule-based system, and also excels in terms of content quality and fluency according to a subjective human test.",
        "Authors": [
            "Lasguido Nio; Rakuten Institute of Technology",
            "Koji Murakami; Rakuten Institute of Technology"
        ],
        "Paper Title": "INTELLIGENCE IS ASKING THE RIGHT QUESTION: A STUDY ON JAPANESE QUESTION GENERATION",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Natural language generation (NLG) is a critical component in spoken dialogue system, which can be divided into two phases: (1) sentence planning: deciding the overall sentence structure, (2) surface realization: determining specific word forms and flattening the sentence structure into a string. With the rise of deep learning, most modern NLG models are based on a sequence-to-sequence (seq2seq) model, which basically contains an encoder-decoder recurrent neural network (RNN); these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization. However, such simple encoder-decoder architecture usually fail to generate complex and long sentences, because the decoder has difficulty learning all grammar and diction knowledge well. This paper introduces an NLG model with a hierarchical attentional decoder, where the hierarchy focuses on leveraging linguistic knowledge in a specific order. The experiments show that the proposed method significantly outperforms the traditional seq2seq model with a smaller model size, and the design of the hierarchical attentional decoder can be applied to various NLG systems. Furthermore, different generation strategies based on linguistic patterns are investigated and discussed in order to guide future NLG research work.",
        "Authors": [
            "Shang-Yu Su; National Taiwan University",
            "Yun-Nung Chen; National Taiwan University"
        ],
        "Paper Title": "INVESTIGATING LINGUISTIC PATTERN ORDERING IN HIERARCHICAL NATURAL LANGUAGE GENERATION",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "This paper addresses the problem of stylized text generation in a multilingual setup. A version of a language model based on a long short-term memory (LSTM) artificial neural network with extended phonetic and semantic embeddings is used for stylized poetry generation. The quality of the resulting poems generated by the network is estimated through bilingual evaluation understudy (BLEU), a survey and a new cross-entropy based metric that is suggested for the problems of such type. The experiments show that the proposed model consistently outperforms random sample and vanilla-LSTM baselines, humans also tend to attribute machine generated texts to the target author.",
        "Authors": [
            "Alexey Tikhonov; Yandex",
            "Ivan Yamshchikov; Max Planck Institute for Mathematics in the Sciences"
        ],
        "Paper Title": "Guess who? Multilingual approach for the automated generation of author-stylized poetry",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Cross-lingual studies attract a growing interest in natural language processing (NLP) research, and several studies showed that similar languages are more advantageous to work with than fundamentally different languages in transferring knowledge. Different similarity measures for the languages are proposed by researchers from different domains. However, a similarity measure focusing on semantic structures of languages can be useful for selecting language pairs or groups to work with, especially for the tasks requiring semantic knowledge such as sentiment analysis or word sense disambiguation. For this purpose, in this study, we leverage a recently proposed word embedding based method to generate a language similarity atlas for 76 different languages around the world. This atlas can help researchers select similar language pairs or groups in cross-lingual applications. Our findings suggest that semantic similarity between two languages is strongly correlated with the geographic proximity of the countries in which they are used.",
        "Authors": [
            "Lütfi Kerem Şenel; ASELSAN",
            "İhsan Utlu; ASELSAN",
            "Veysel Yücesoy; ASELSAN",
            "Aykut Koç; ASELSAN",
            "Tolga Çukur; Bilkent University"
        ],
        "Paper Title": "Generating Semantic Similarity Atlas for Natural Languages",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Natural language processing:"
    },
    {
        "Abstract": "Images may have elements containing text and a bounding box associated with them, for example, text identified via optical character recognition on a computer screen image, or a natural image with labeled objects. We present an end-to-end trainable architecture to incorporate the information from these elements and the image to segment/identify the part of the image a natural language expression is referring to. We calculate an embedding for each element and then project it onto the corresponding location (i.e., the associated bounding box) of the image feature map. We show that this architecture gives an improvement in resolving referring expressions, over only using the image, and other methods that incorporate the element information. We demonstrate experimental results on the referring expression datasets based on COCO, and on a webpage image referring expression dataset that we developed.",
        "Authors": [
            "Nevan Wichers; Google AI",
            "Dilek Hakkani-Tür; Google AI",
            "Jindong (JD) Chen; Google AI"
        ],
        "Paper Title": "RESOLVING REFERRING EXPRESSIONS IN IMAGES WITH LABELED ELEMENTS",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    },
    {
        "Abstract": "Sentiment classification on spoken language transcriptions has received less attention. A practical system employing the spoken language modality will have to use a language transcription from an Automatic Speech Recognition (ASR) engine which is inherently prone to errors. The main interest of this paper lies in improvement of sentiment classification on erroneous ASR transcriptions. Our aim is to improve the representations of the ASR transcripts using manual transcripts and other modalities, like audio and visual, that are available during training. We adopt an approach based on Deep Canonical Correlation Analysis (DCCA) and propose two new extensions of DCCA to enhance the ASR view using multiple modalities. We present a detailed evaluation of the performance of our approach on datasets of opinion videos (CMU-MOSI and CMU-MOSEI) collected from Youtube.",
        "Authors": [
            "Sri Harsha Dumpala; TCS Research and Innovation-Mumbai",
            "Imran Sheikh; TCS Research and Innovation-Mumbai",
            "Rupayan Chakraborty; TCS Research and Innovation-Mumbai",
            "Sunil Kumar Kopparapu; TCS Research and Innovation-Mumbai"
        ],
        "Paper Title": "SENTIMENT CLASSIFICATION ON ERRONEOUS ASR TRANSCRIPTS: A MULTI VIEW LEARNING APPROACH",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "Natural Language Processing",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    },
    {
        "Abstract": "Multidialectal languages can pose challenges for acoustic modeling. Past research has shown that with a large training corpus but without explicit modeling of inter-dialect variability, training individual per-dialect models yields superior performance to that of a single model trained on the combined data~\\cite{li2018multi, diak}. Our goal was thus to create a single multidialect acoustic model that would rival the performance of the dialect-specific models. Working in the context of deep Long-Short Term Memory (LSTM) acoustic models trained on up to 40K hours of speech, we explored several methods for training and incorporating dialect-specific information into the model, including 12 variants of interpolation-of-bases techniques related to Cluster Adaptive Training (CAT) and Factorized Hidden Layer (FHL) techniques. We found that with our model topology and large training corpus, simply appending the dialect-specific information to the feature vector resulted in a more accurate model than any of the more complex interpolation-of-bases techniques, while requiring less model complexity and fewer parameters. This simple adaptation yielded a single unified model for all dialects that, in most cases, outperformed individual models which had been trained per-dialect.",
        "Authors": [
            "Mikaela Grace; Google",
            "Meysam Bastani; Google",
            "Eugene Weinstein; Google"
        ],
        "Paper Title": "Occam's Adaptation: A Comparison of Interpolation of Bases Adaptation Methods for Multi-Dialect Acoustic Modeling with LSTMs",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Code-switching speech, in which speakers alternate between two or more languages in the same utterance often occur in bilingual and multilingual communities. Such phenomenon poses challenges for spoken language technologies, i.e., automatic speech recognition (ASR) and text-to-speech synthesis (TTS), as the systems need to be able to handle the input in a multilingual setting. We may find code-switching text or code-switching speech in social media, but parallel speech and transcription of code-switching data suitable for training ASR and TTS are mostly unavailable. In this paper, we utilize a speech chain framework based on deep learning to enable ASR and TTS to learn code-switching in semi-supervised fashion. In particular, we construct our system on Japanese-English conversational speech. We first train ASR and TTS systems separately with parallel speech-text of monolingual data (supervised learning) and perform speech chain with only code-switching text or code-switching speech (unsupervised learning). Experimental results reveal that such closed-loop architecture allows ASR and TTS to teach each other without any parallel of code-switching data, and successfully improve the performance closed to the system that trained with sizeable parallel text-speech of code-switching data.",
        "Authors": [
            "Sahoko Nakayama; Nara Institute of Science and Technology",
            "Andros Tjandra; Nara Institute of Science and Technology",
            "Sakriani Sakti; Nara Institute of Science and Technology",
            "Satoshi Nakamura; Nara Institute of Science and Technology"
        ],
        "Paper Title": "SPEECH CHAIN FOR SEMI-SUPERVISED LEARNING OF JAPANESE-ENGLISH CODE-SWITCHING ASR AND TTS",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Applications of automatic speech recognition (ASR) such as broadcast transcription and dialog systems, can be helped by the ability to detect errors in the ASR output. The field of ASR error detection has emerged as a way to detect and subsequently correct ASR errors. The most common approach for ASR error detection is features-based, where a set of features are extracted from the ASR output and used to train a classifier to predict correct/incorrect labels. Language models (LMs), either from the ASR decoder or externally trained, can be used to provide features to an ASR error detection system, through scores computed on the ASR output. Recently, recurrent neural network language models (RNNLMs) features were proposed for ASR error detection with improvements to the classification rate, thanks to their ability to model longer-range context. RNNLM adaptation, through the introduction of auxiliary features that encode domain, has been shown to improve ASR performance. This work investigates whether RNNLM adaptation techniques can also improve ASR error detection performance in the context of multi-genre broadcast ASR. The results show that an overall improvement of about 1% in the F-measure can be achieved using adapted RNNLM features.",
        "Authors": [
            "Rahhal Errattahi; University of Chouaib Doukkali",
            "Salil Deena; The University of Sheffield",
            "Asmaa El Hannani; University of Chouaib Doukkali",
            "Hassan Ouahmane; University of Chouaib Doukkali",
            "Thomas Hain; The University of Sheffield"
        ],
        "Paper Title": "Improving ASR Error Detection with RNNLM Adaptation",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "The cross entropy (CE) loss function is commonly adopted for neural network language model (NNLM) training. Although this criterion is largely successful, as evidenced by the quick advance of NNLM, minimizing CE only maximizes likelihood of training data. When training data is insufficient, the generalization power of the resulting LM is limited on test data. In this paper, we propose to integrate a pairwise ranking (PR) loss with the CE loss for multi-objective training on recurrent neural network language model (RNNLM). The PR loss emphasizes discrimination between target and non-target words and also reserves probabilities for low-frequency correct words, which complements the distribution learning role of the CE loss. Combining the two losses may therefore help improve the performance of RNNLM. In addition, we incorporate multi-task learning (MTL) into the proposed multi-objective learning to regularize the primary task of RNNLM by an auxiliary task of part-of-speech (POS) tagging. The proposed approach to RNNLM learning has been evaluated on two speech recognition tasks of WSJ and AMI with encouraging results achieved on word error rate reductions.",
        "Authors": [
            "Minguang Song; University of Missouri",
            "Yunxin Zhao; University of Missouri",
            "Shaojun Wang; Ping An Technology"
        ],
        "Paper Title": "MULTI-OBJECTIVE MULTI-TASK LEARNING ON RNNLM FOR SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "The standard approach to assess reliability of automatic speech transcriptions is through the use of confidence scores. If accurate, these scores provide a flexible mechanism to flag transcription errors for upstream and downstream applications. One challenging type of errors that recognisers make are deletions. These errors are not accounted for by the standard confidence estimation schemes and are hard to rectify in the upstream and downstream processing. High deletion rates are prominent in limited resource and highly mismatched training/testing conditions studied under IARPA Babel and Material programs. This paper looks at the use of bidirectional recurrent neural networks to yield confidence estimates in predicted as well as deleted words. Several simple schemes are examined for combination. To assess usefulness of this approach, the combined confidence score is examined for untranscribed data selection that favours transcriptions with lower deletion errors. Experiments are conducted using IARPA Babel/Material program languages.",
        "Authors": [
            "Anton Ragni; University of Cambridge",
            "Qiujia Li; University of Cambridge",
            "Mark Gales; University of Cambridge",
            "Yu Wang; University of Cambridge"
        ],
        "Paper Title": "CONFIDENCE ESTIMATION AND DELETION PREDICTION USING BIDIRECTIONAL RECURRENT NEURAL NETWORKS",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Acoustic-to-word (A2W) end-to-end automatic speech recognition (ASR) systems have attracted attention because of an extremely simplified architecture and fast decoding. To alleviate data sparseness issues due to infrequent words, the combination with an acoustic-to-character (A2C) model is investigated. Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words. A2W models learn contexts with both acoustic and transcripts; therefore they tend to falsely recognize OOV words as words in the vocabulary. In this paper, we tackle this problem by using external language models (LM), which are trained only with transcriptions and have better linguistic information to detect OOV words. The A2C model is used to resolve these OOV words. Experimental evaluations show that external LMs have the effects of not only reducing errors but also increasing the number of detected OOV words, and the proposed method significantly improves performances in English conversational and Japanese lecture corpora, especially for out-of-domain scenario. We also investigate the impact of the vocabulary size of A2W models and data size for training LMs. Moreover, our approach can reduce the vocabulary size several times with marginal performance degradation.",
        "Authors": [
            "Hirofumi Inaguma; Kyoto University",
            "Masato Mimura; Kyoto University",
            "Shinsuke Sakai; Kyoto University",
            "Tatsuya Kawahara; Kyoto University"
        ],
        "Paper Title": "IMPROVING OOV DETECTION AND RESOLUTION WITH EXTERNAL LANGUAGE MODELS IN ACOUSTIC-TO-WORD ASR",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Transcription of multimedia data sources is often a challenging automatic speech recognition (ASR) task. The incorporation of visual features as additional contextual information as a means to improve ASR for this data has recently drawn attention from researchers. Our investigation extends existing work by using images and video titles to adapt a recurrent neural network (RNN) language model with long-short term memory (LSTM). Our language model is tested on an existing corpus of instruction videos and on a new corpus consisting of lecture videos. Consistent reduction in perplexity by 5-10 was observed on both datasets. When the non-adapted model was combined with the image adaptation and video title adaptation models for n-best hypotheses re-ranking, word error rate (WER) is decreased by around 0.5% on the both datasets. By analysing output word probabilities of the model, it was found that both image adaptation and video title adaptation give the model more confidence in choice of contextually correct words.",
        "Authors": [
            "Yasufumi Moriya; Dublin City University",
            "Gareth Jones; Dublin City University"
        ],
        "Paper Title": "LSTM LANGUAGE MODEL ADAPTATION WITH IMAGES AND TITLES FOR MULTIMEDIA AUTOMATIC SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    },
    {
        "Abstract": "In speech recognition of morphologically rich languages, very large vocabulary sizes are required to achieve good error rates. Especially traditional n-gram language models trained over word sequences suffer from data sparsity issues. The language modelling can often be improved by segmenting the words to sequences of subword units that are more frequent. Another solution is to cluster the words into classes and apply a class-based language model. We show that linearly interpolating n-gram models trained over words, subwords, and word classes improves the first-pass speech recognition accuracy in very large vocabulary speech recognition tasks for two morphologically rich and agglutinative languages, Finnish and Estonian. To overcome performance issues, we also introduce a novel language model look-ahead method utilizing a class bigram model. The method improves the results over a unigram look-ahead model with the same recognition speed, the difference increasing for small real-time factors. The improved model combination and look-ahead model are useful in cases where real-time recognition is required or when the improved hypotheses help with further recognition passes. For instance, neural network language models are mostly applied by rescoring the generated hypotheses due to higher computational costs.",
        "Authors": [
            "Matti Varjokallio; Aalto University",
            "Sami Virpioja; Aalto University",
            "Mikko Kurimo; Aalto University"
        ],
        "Paper Title": "FIRST-PASS TECHNIQUES FOR VERY LARGE VOCABULARY SPEECH RECOGNITION OF MORPHOLOGICALLY RICH LANGUAGES",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We explore why deep convolutional neural networks (CNNs) with small two-dimensional kernels, primarily used for modeling spatial relations in images, are also effective in speech recognition. We analyze the representations learned by deep CNNs and compare them with deep neural network (DNN) representations and i-vectors, in the context of acoustic model adaptation. To explore whether interpretable information can be decoded from the learned representations we evaluate their ability to discriminate between speakers, acoustic conditions, noise type, and gender using the Aurora-4 dataset. We extract both whole model embeddings (to capture the information learned across the whole network) and layer-specific embeddings which enable understanding of the flow of information across the network. We also use learned representations as the additional input for a time-delay neural network (TDNN) for the Aurora-4 and MGB-3 English datasets. We find that deep CNN embeddings outperform DNN embeddings for acoustic model adaptation and auxiliary features based on deep CNN embeddings result in similar word error rates to i-vectors.",
        "Authors": [
            "Joanna Rownicka; The University of Edinburgh",
            "Peter Bell; The University of Edinburgh",
            "Steve Renals; The University of Edinburgh"
        ],
        "Paper Title": "ANALYZING DEEP CNN-BASED UTTERANCE EMBEDDINGS FOR ACOUSTIC MODEL ADAPTATION",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Spectro-temporal feature extraction and multi-band processing were both invented with the goal of increasing the robustness of speech recognisers. However, although these methods have been in use for a long time now, and they are evidently compatible, few attempts have been made to combine them. This is why here we investigate the combination of multi-band processing with the use of spectro-temporal Gabor filters. First, based on the TIMIT corpus, we optimise their meta-parameters like the overlap, and the number of bands. Then we verify the cross-corpus viability of our multi-band processing approach on the Aurora-4 corpus. Lastly, we combine our method with the recently proposed channel dropout method. Our results show that this combination not only leads to lower error rates than those got using either multi-band processing or channel dropout, but these results compare favourably to those recently reported for the clean training scenario on the Aurora-4 corpus.",
        "Authors": [
            "György Kovács; MTA-SZTE Research Group on Artificial Intelligence",
            "László Tóth; University of Szeged",
            "Gábor Gosztolya; MTA-SZTE Research Group on Artificial Intelligence"
        ],
        "Paper Title": "MULTI-BAND PROCESSING WITH GABOR FILTERS AND TIME DELAY NEURAL NETS FOR NOISE ROBUST SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Teacher-student (T-S) learning is a transfer learning approach, where a teacher network is used to teach a student network to make the same predictions as the teacher. Originally formulated for model compression, this approach has also been used for domain adaptation, and is particularly effective when parallel data is available in source and target domains. The standard approach uses a frame-level objective of minimizing the KL divergence between the frame-level posteriors of the teacher and student networks. However, for sequence-trained models for speech recognition, it is more appropriate to train the student to mimic the sequence-level posterior of the teacher network. In this work, we compare this sequence-level KL divergence objective with another semi-supervised sequence-training method, namely the lattice-free MMI, for unsupervised domain adaptation. We investigate the approaches in multiple scenarios including adapting from clean to noisy speech, bandwidth mismatch and channel mismatch.",
        "Authors": [
            "Vimal Manohar; Johns Hopkins University",
            "Pegah Ghahremani; Johns Hopkins University",
            "Daniel Povey; Johns Hopkins University",
            "Sanjeev Khudanpur; Johns Hopkins University"
        ],
        "Paper Title": "A teacher-student learning approach for unsupervised domain adaptation of sequence-trained ASR models",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Developing a practical speech recognizer for a low resource language is challenging, not only because of the (potentially unknown) properties of the language, but also because test data may not be from the same domain as the available training data. In this paper, we focus on the latter challenge, i.e. domain mismatch, for systems trained using a sequence-based criterion. We demonstrate the effectiveness of using a pre-trained English recognizer, which is robust to such mismatched conditions, as a domain normalizing feature extractor on a low resource language. In our example, we use Turkish Conversational Speech and Broadcast News data. This enables rapid development of speech recognizers for new languages which can easily adapt to any domain. Testing in various cross-domain scenarios, we achieve relative improvements of around 25% in phoneme error rate, with improvements being around 50% for some domains.",
        "Authors": [
            "Siddharth Dalmia; Carnegie Mellon University",
            "Xinjian Li; Carnegie Mellon University",
            "Florian Metze; Carnegie Mellon University",
            "Alan W Black; Carnegie Mellon University"
        ],
        "Paper Title": "DOMAIN ROBUST FEATURE EXTRACTION FOR RAPID LOW RESOURCE ASR DEVELOPMENT",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Thursday, December 20, 13:30 - 15:30",
        "Session": "ASR II",
        "Session Time": "Thursday, December 20, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it (1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic sounding speech. Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.",
        "Authors": [
            "Hirokazu Kameoka; NTT Corporation",
            "Takuhiro Kaneko; NTT Corporation",
            "Kou Tanaka; NTT Corporation",
            "Nobukatsu Hojo; NTT Corporation"
        ],
        "Paper Title": "StarGAN-VC: Non-parallel many-to-many voice conversion using star generative adversarial networks",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Speaking rate refers to the average number of phonemes within some unit time, while the rhythmic patterns refer to duration distributions for realizations of different phonemes within different phonetic structures. Both are key components of prosody in speech, which is different for different speakers. Models like cycle-consistent adversarial network (Cycle-GAN) and variational auto-encoder (VAE) have been successfully applied to voice conversion tasks without parallel data. However, due to the neural network architectures and feature vectors chosen for these approaches, the length of the predicted utterance has to be fixed to that of the input utterance, which limits the flexibility in mimicking the speaking rates and rhythmic patterns for the target speaker. On the other hand, sequence-to-sequence learning model was used to remove the above length constraint, but parallel training data are needed. In this paper, we propose an approach utilizing sequence-to-sequence model trained with unsupervised Cycle-GAN to perform the transformation between the phoneme posteriorgram sequences for different speakers. In this way, the length constraint mentioned above is removed to offer rhythm-flexible voice conversion without requiring parallel data. Preliminary evaluation on two datasets showed very encouraging results.",
        "Authors": [
            "Cheng-chieh Yeh; National Taiwan University",
            "Po-chun Hsu; National Taiwan University",
            "Ju-chieh Chou; National Taiwan University",
            "Hung-yi Lee; National Taiwan University",
            "Lin-shan Lee; National Taiwan University"
        ],
        "Paper Title": "RHYTHM-FLEXIBLE VOICE CONVERSION WITHOUT PARALLEL DATA USING CYCLE-GAN OVER PHONEME POSTERIORGRAM SEQUENCES",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper, we propose to use generative adversarial networks (GAN) together with a WaveNet vocoder to address the over-smoothing problem arising from the deep learning approaches to voice conversion, and to improve the vocoding quality over the traditional vocoders. As GAN aims to minimize the divergence between the natural and converted speech parameters, it effectively alleviates the over-smoothing problem in the converted speech. On the other hand, WaveNet vocoder allows us to leverage from the human speech of a large speaker population, thus improving the naturalness of the synthetic voice. Furthermore, for the first time, we study how to use WaveNet vocoder for residual compensation to improve the voice conversion performance. The experiments show that the proposed voice conversion framework consistently outperforms the baselines.",
        "Authors": [
            "Berrak Sisman; National University of Singapore",
            "Mingyang Zhang; National University of Singapore",
            "Sakriani Sakti; Nara Institute of Science and Technology",
            "Haizhou Li; National University of Singapore",
            "Satoshi Nakamura; Nara Institute of Science and Technology"
        ],
        "Paper Title": "Adaptive WaveNet Vocoder for Residual Compensation in GAN-based Voice Conversion",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "Recently, speaker adaptation of neural TTS models received significant interest, and several studies focusing on this topic have been published. All of them explore an adaptation of an initial multi-speaker model trained on a corpus containing from tens to hundreds of individual speaker voices. In this work we focus on a challenging task of TTS voice conversion where an initial system is trained on a single-speaker data and then need to be adapted to a variety of external speaker voices. The TTS voice conversion setup represents a very important use case. Transcribed multi-speaker datasets might be unavailable for many languages while any TTS technology provider is expected to have at least one suitable single-speaker dataset per supported language. We present a neural TTS system comprising separate prosody generator and synthesizer DNN models. The system is trained on a high quality proprietary male speaker dataset. We show that the system models can be converted to a variety of external male and female ordinary voices and an extremely expressive artist’s voice and present crowd-base subjective evaluation results.",
        "Authors": [
            "Zvi Kons; IBM Research",
            "Slava Shechtman; IBM Research",
            "Alex Sorin; IBM Research",
            "Ron Hoory; IBM Research",
            "Carmel Rabinovitz; IBM Research",
            "Edmilson Da Silva Morais; IBM Research"
        ],
        "Paper Title": "NEURAL TTS VOICE CONVERSION",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "This paper presents an evaluation of deep spectral mapping and WaveNet vocoder in voice conversion (VC). In our VC framework, spectral features of an input speaker are converted into those of a target speaker using the deep spectral mapping, and then together with the excitation features, the converted waveform is generated using WaveNet vocoder. In this work, we compare three different deep spectral mapping networks, i.e., a deep single density network (DSDN), a deep mixture density network (DMDN), and a long short-term memory recurrent neural network with an autoregressive output layer (LSTM-AR). Moreover, we also investigate several methods for reducing mismatches of spectral features used in WaveNet vocoder between training and conversion processes, such as some methods to alleviate oversmoothing effects of the converted spectral features, and another method to refine WaveNet using the converted spectral features. The experimental results demonstrate that the LSTM-AR yields nearly better spectral mapping accuracy than the others, and the proposed WaveNet refinement method significantly improves the naturalness of the converted waveform.",
        "Authors": [
            "Patrick Lumban Tobing; Nagoya University",
            "Tomoki Hayashi; Nagoya University",
            "Yi-Chiao Wu; Nagoya University",
            "Kazuhiro Kobayashi; Nagoya University",
            "Tomoki Toda; Nagoya University"
        ],
        "Paper Title": "AN EVALUATION OF DEEP SPECTRAL MAPPINGS AND WAVENET VOCODER FOR VOICE CONVERSION",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Compared with WaveNet vocoder, FFTNet vocoder can synthesize speech waveforms in real time but the synthesized speech quality is not so high. To improve the synthesized speech quality of FFTNet neural vocoder while keeping the network model size for real-time synthesis, this paper provides the following four approaches. 1) The residual connections are introduced into FFTNet for improving the prediction accuracy. 2) Noise shaping and 3) subband approaches which can significantly improve the synthesized speech quality in WaveNet vocoder are directly applied to FFTNet vocoder. 4) Subband FFTNet vocoder with multiband input is additionally proposed for directly compensating the phase shift between subbands. The proposed approaches are evaluated by both objective and subjective experiments using a Japanese male corpus with a sampling frequency of 16 kHz compared with STRAIGHT without mel-cepstral compression, vanilla FFTNet and WaveNet vocoders. The results indicate that the proposed approaches can successfully improve the synthesized speech quality of FFTNet vocoder. Especially, the proposal with noise shaping significantly outperforms the STRAIGHT.",
        "Authors": [
            "Takuma Okamoto; National Institute of Information and Communications Technology",
            "Tomoki Toda; Nagoya University",
            "Yoshinori Shiga; National Institute of Information and Communications Technology",
            "Hisashi Kawai; National Institute of Information and Communications Technology"
        ],
        "Paper Title": "IMPROVING FFTNET VOCODER WITH NOISE SHAPING AND SUBBAND APPROACHES",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "ToBI is the dominant tool for symbolically describing prosodic content in American English speech material. This is due to its descriptive power and its theoretical grounding, but also to the amount of available annotated data. Recently, a modest amount of material annotated with the Rhythm and Pitch (RaP) framework was released publicly. In this paper, we investigate the acoustic-symbolic relationship under these two systems. We present experiments looking at this relationship in both directions. From acoustic to symbolic, we compare the automatic prediction of prosodic prominence as defined under the two systems. From symbolic to acoustic, we examine the utility of these annotation standards to correctly prescribe the acoustics of a given utterance from their symbolic sequences. We find RaP to be promising, showing a somewhat stronger acoustic-symbolic relationship than ToBI given a comparable amount of data for some aspects of these tasks. While with more annotated data ToBI results are stronger, it remains to be shown whether RaP performance can scale up.",
        "Authors": [
            "Raul Fernandez; IBM Research",
            "Andrew Rosenberg; IBM Research"
        ],
        "Paper Title": "COMPARING PROSODIC FRAMEWORKS: INVESTIGATING THE ACOUSTIC-SYMBOLIC RELATIONSHIP IN TOBI AND RAP",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This work investigates techniques that select training data from small, found corpuses in order to improve the naturalness of synthesized text-to-speech voices. The approach outlined in this paper examines different metrics to detect and reject segments of training data that can degrade the performance of the system. We conducted experiments on two small datasets extracted from Mandarin Chinese audiobooks that have different characteristics in terms of recording conditions, narrator, and transcriptions. We show that using a even smaller, yet carefully selected, set of data can lead to a text-to-speech system able to generate more natural speech than a system trained on the complete dataset. Three metrics related to the narrator's articulation proposed in the paper give significant improvements in naturalness.",
        "Authors": [
            "Fang-Yu Kuo; ObEN, Inc.",
            "Sandesh Aryal; ObEN, Inc.",
            "Gilles Degottex; ObEN, Inc.",
            "Sam Kang; ObEN, Inc.",
            "Pierre Lanchantin; ObEN, Inc.",
            "Iris Ouyang; ObEN, Inc."
        ],
        "Paper Title": "DATA SELECTION FOR IMPROVING NATURALNESS OF TTS VOICES TRAINED ON SMALL FOUND CORPUSES",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Statistical TTS systems that directly predict the speech waveform have recently reported improvements in synthesis quality. This investigation evaluates Amazon’s statistical speech waveform synthesis (SSWS) system. An in-depth evaluation of SSWS is conducted across a number of domains to better understand the consistency in quality. The results of this evaluation are validated by repeating the procedure on a separate group of testers. Finally, an analysis of the nature of speech errors of SSWS compared to hybrid unit selection synthesis is conducted to identify the strengths and weaknesses of SSWS. Having a deeper insight into SSWS allows us to better define the focus of future work to improve this new technology.",
        "Authors": [
            "Thomas Merritt; Amazon",
            "Bartosz Putrycz; Amazon",
            "Adam Nadolski; Amazon",
            "Tianjun Ye; Amazon",
            "Daniel Korzekwa; Amazon",
            "Wiktor Dolecki; Amazon",
            "Thomas Drugman; Amazon",
            "Viacheslav Klimkov; Amazon",
            "Alexis Moinet; Amazon",
            "Andrew Breen; Amazon",
            "Rafal Kuklinski; Amazon",
            "Nikko Strom; Amazon",
            "Roberto Barra-Chicote; Amazon"
        ],
        "Paper Title": "COMPREHENSIVE EVALUATION OF STATISTICAL SPEECH WAVEFORM SYNTHESIS",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper presents a hybrid text-to-speech framework that uses a waveform generation method based on examplars of natural speech waveform. These examplars are selected at synthesis time given a sequence of acoustic features generated from text by a statistical parametric speech synthesis model. In order to match the expected degradation of these target synthesis features, the database of units is constructed such that the units' target representations are generated from the same parametric model. We evaluate two variants of this framework by modifying the size of the examplar: a small unit variant (where unit boundaries are determined by pitch mark location) and a halfphone variant (where unit boundaries are determined by subphone state forced alignment). We found that for a larger dataset (around four hours of training data) the examplar-based waveform generation variants are rated higher than the vocoder-based system.",
        "Authors": [
            "Cassia Valentini-Botinhao; University of Edinburgh",
            "Oliver Watts; University of Edinburgh",
            "Felipe Espic; University of Edinburgh",
            "Simon King; University of Edinburgh"
        ],
        "Paper Title": "EXAMPLAR-BASED SPEECH WAVEFORM GENERATION FOR TEXT-TO-SPEECH",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Special session on Speech Synthesis:"
    },
    {
        "Abstract": "This paper describes an Icelandic pronunciation dictionary for speech applications and its processing for use in a text-to-speech system for Icelandic. Cleaning and correction procedures were implemented to create a consistent training set for grapheme-to-phoneme conversion modeling, needed for the automatic extension of the dictionary. Experiments with the original version of the dictionary and the cleaned version described in this paper as training sets for a joint sequence g2p algorithm show a clear benefit of using clean data for training, both in terms of PER and in terms of categories of errors made by the g2p algorithm. The results of the dictionary processing where also used to create an initial version of an open source database for Icelandic speech applications.",
        "Authors": [
            "Anna Björk Nikulásdóttir; Reykjavik University",
            "Jón Guðnason; Reykjavik University",
            "Eiríkur Rögnvaldsson; University of Iceland"
        ],
        "Paper Title": "AN ICELANDIC PRONUNCIATION DICTIONARY FOR TTS",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper reconsiders the use of MOS naturalness as an instrument for measuring the quality (vs.\\ intelligibility) of speech. We reconsider an earlier proposed alternative, the paired comparison or \"AB\" test, and present new empirical evidence that this is indeed a better method for evaluating TTS quality. Using this, we evaluate three older TTS systems along with a recent deep-learning approach against native North-American and Indian speech and show that, in fact, TTS had already crossed the threshold of human-like speech synthesis some time ago. This suggests that a systematic reappraisal of the concept of abstract ``naturalness'' of speech is in order.",
        "Authors": [
            "Sajad Shirali-Shahreza; University of Toronto",
            "Gerald Penn; University of Toronto"
        ],
        "Paper Title": "MOS NATURALNESS AND THE QUEST FOR HUMAN-LIKE SPEECH",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "Voice Conversion and TTS",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Evaluation methodologies:"
    },
    {
        "Abstract": "Acoustic model and language model (LM) have been two major components in conventional speech recognition systems. They are normally trained independently, but recently there has been a trend to optimize both components simultaneously in a unified end-to-end (E2E) framework. However, the performance gap between the E2E systems and the traditional hybrid systems suggests that some knowledge has not yet been fully utilized in the new framework. An observation is that the current attention-based E2E systems could produce better recognition results when decoded with LMs which are independently trained with the same resource. In this paper, we focus on how to improve attention-based E2E systems without increasing model complexity or resorting to extra data. A novel training strategy is proposed for multi-task training with the connectionist temporal classification (CTC) loss. The sequence-based minimum Bayes risk (MBR) loss is also investigated. Our experiments on SWB 300hrs showed that both loss functions could significantly improve the baseline model performance. The additional gain from joint-LM decoding remains the same for CTC trained model but is only marginal for MBR trained model. This implies that while CTC loss function is able to capture more acoustic knowledge, MBR loss function exploits more lexicon dependency.",
        "Authors": [
            "Jia Cui; Tencent AI Lab",
            "Chao Weng; Tencent AI Lab",
            "Guangsen Wang; Tencent AI Lab",
            "Jun Wang; Tencent AI Lab",
            "Peidong Wang; The Ohio State University",
            "Chengzhu Yu; Tencent AI Lab",
            "Dan Su; Tencent AI Lab",
            "Dong Yu; Tencent AI Lab"
        ],
        "Paper Title": "IMPROVING ATTENTION-BASED END-TO-END ASR SYSTEMS WITH SEQUENCE-BASED LOSS FUNCTIONS",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper, we develop an end-to-end automatic speech recognition (ASR) model designed for a common low-resource scenario: no pronunciation dictionary or phonemic transcripts, very limited transcribed speech, and much larger non-parallel text and speech corpora. Our semi-supervised model is built on top of an encoder decoder model with attention and takes advantage of non-parallel speech and text corpora in several ways: a denoising text autoencoder that shares parameters with the ASR decoder, a speech autoencoder that shares parameters with the ASR encoder, and adversarial training that encourages the speech and text encoders to use the same embedding space. We show that a model with this architecture significantly outperforms the baseline in this low-resource condition. We additionally perform an ablation evaluation, demonstrating that all of our added components contribute substantially to the overall performance of our model. We propose several avenues for further work, noting in particular that a model with this architecture could potentially enable fully unsupervised speech recognition.",
        "Authors": [
            "Jennifer Drexler; Massachusetts Institute of Technology",
            "James Glass; Massachusetts Institute of Technology"
        ],
        "Paper Title": "COMBINING END-TO-END AND ADVERSARIAL TRAINING FOR LOW-RESOURCE SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Attention-based recurrent neural encoder-decoder models present an elegant solution to the automatic speech recognition problem. This approach folds the acoustic model, pronunciation model, and language model into a single network and requires only a parallel corpus of speech and text for training. However, unlike in conventional approaches that combine separate acoustic and language models, it is not clear how to use additional (unpaired) text. While there has been previous work on methods addressing this problem, a thorough comparison among methods is still lacking. In this paper, we compare a suite of past methods and some of our own proposed methods for using unpaired text data to improve encoder-decoder models. For evaluation, we use the medium-sized Switchboard data set and the large-scale Google voice search and dictation data sets. Our results confirm the benefits of using unpaired text across a range of methods and data sets. Surprisingly, for first-pass decoding, the rather simple approach of shallow fusion performs best across data sets. However, for Google data sets we find that cold fusion has a lower oracle error rate and outperforms other approaches after second-pass rescoring on the Google voice search data set.",
        "Authors": [
            "Shubham Toshniwal; Toyota Technological Institute at Chicago",
            "Anjuli Kannan; Google",
            "Chung-Cheng Chiu; Google",
            "Yonghui Wu; Google",
            "Tara N. Sainath; Google",
            "Karen Livescu; Toyota Technological Institute at Chicago"
        ],
        "Paper Title": "A COMPARISON OF TECHNIQUES FOR LANGUAGE MODEL INTEGRATION IN ENCODER-DECODER SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Most of the current automatic speech recognition is performed on a remote server. However, the demand for speech recognition on personal devices is increasing, owing to the requirement of shorter recognition latency and increased privacy. End-to-end speech recognition that employs recurrent neural networks (RNNs) shows good accuracy, but the execution of conventional RNNs, such as the long short-term memory (LSTM) or gated recurrent unit (GRU), demands many memory accesses, thus hindering its real-time execution on smart-phones or embedded systems. To solve this problem, we built an end-to-end acoustic model (AM) using linear recurrent units instead of LSTM or GRU and employed a multi-step parallel approach for reducing the number of DRAM accesses. The AM is trained with the connectionist temporal classification (CTC) loss, and the decoding is conducted using weighted finite-state transducers (WFSTs). The proposed system achieves x4.8 real-time speed when executed on a single core of an ARM CPU-based system.",
        "Authors": [
            "Yoonho Boo; Seoul National University",
            "Jinhwan Park; Seoul National University",
            "Lukas Lee; Seoul National University",
            "Wonyong Sung; Seoul National University"
        ],
        "Paper Title": "ON-DEVICE END-TO-END SPEECH RECOGNITION WITH MULTI-STEP PARALLEL RNNS",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "End-to-end automatic speech recognition (ASR) has simplified the traditional ASR system building pipeline by eliminating the need to have multiple components and also the requirement for expert linguistic knowledge for creating pronunciation dictionaries. Therefore, end-to-end ASR fits well when building systems for new domains. However, one major drawback of end-to-end ASR is that, it is necessary to have a larger amount of labeled speech in comparison to traditional methods. Therefore, in this paper, we explore domain adaptation approaches for end-to-end ASR in low-resource settings. We show that joint domain identification and speech recognition by inserting a symbol for domain at the beginning of the label sequence, factorized hidden layer adaptation and a domain-specific gating mechanism improve the performance for a low-resource target domain. Furthermore, we also show the robustness of proposed adaptation methods to an unseen domain, when only 3 hours of untranscribed data is available with improvements reporting upto 8.7% relative.",
        "Authors": [
            "Lahiru Samarakoon; Fano Labs",
            "Brian Mak; Hong Kong University of Science and Technology",
            "Albert Lam; Fano Labs"
        ],
        "Paper Title": "DOMAIN ADAPTATION OF END-TO-END SPEECH RECOGNITION IN LOW-RESOURCE SETTINGS",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper investigates the impact of word-based RNN language models (RNN-LMs) on the performance of end-to-end automatic speech recognition (ASR). In our prior work, we have proposed a multi-level LM, in which character-based and word-based RNN-LMs are combined in hybrid CTC/attention-based ASR. Although this multi-level approach achieves significant error reduction in the Wall Street Journal (WSJ) task, two different LMs need to be trained and used for decoding, which increase the computational cost and memory usage. In this paper, we further propose a novel word-based RNN-LM, which allows us to decode with only the word-based LM, where it provides look-ahead word probabilities to predict next characters instead of the character-based LM, leading competitive accuracy with less computation compared to the multi-level LM. We demonstrate the efficacy of the word-based RNN-LMs using a larger corpus, LibriSpeech, in addition to WSJ we used in the prior work. Furthermore, we show that the proposed model achieves 5.1 \\%WER for WSJ Eval'92 test set when the vocabulary size is increased, which is the best WER reported for end-to-end ASR systems on this benchmark.",
        "Authors": [
            "Takaaki Hori; Mitsubishi Electric Research Laboratories",
            "Jaejin Cho; Johns Hopkins University",
            "Shinji Watanabe; Johns Hopkins University"
        ],
        "Paper Title": "END-TO-END SPEECH RECOGNITION WITH WORD-BASED RNN LANGUAGE MODELS",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Acoustic-to-Word recognition provides a straightforward solution to end-to-end speech recognition without needing external decoding, language model re-scoring or lexicon. While character-based models offer a natural solution to the out-of-vocabulary problem, word models can be simpler to decode and may also be able to directly recognize semantically meaningful units. We present effective methods to train Sequence-to-Sequence models for direct word-level recognition (and character-level recognition) and show an absolute improvement of 4.4-5.0\\% in Word Error Rate on the Switchboard corpus compared to prior work. In addition to these promising results, word-based models are more interpretable than character models, which have to be composed into words using a separate decoding step. We analyze the encoder hidden states and the attention behavior, and show that location-aware attention naturally represents words as a single speech-word-vector, despite spanning multiple frames in the input. We finally show that the Acoustic-to-Word model also learns to segment speech into words with a mean standard deviation of 3 frames as compared with human annotated forced-alignments for the Switchboard corpus.",
        "Authors": [
            "Shruti Palaskar; Carnegie Mellon University",
            "Florian Metze; Carnegie Mellon University"
        ],
        "Paper Title": "ACOUSTIC-TO-WORD RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODELS",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper, we propose an end-to-end noise-robust automatic speech recognition system through deep-learning implementation of de-noising auto-encoders and recurrent neural networks. We use batch normalization and a novel design for the front-end de-noising auto-encoder, which mimics a two-stage prediction of a single-frame clean feature vector from multi-frame noisy feature vectors. For the back-end word recognition, we use an end-to-end system based on bidirectional recurrent neural network with long short-term memory cells. The LSTM-BiRNN is trained via connectionist temporal classification criterion. Its performance is compared to a baseline backend based on hidden Markov models and Gaussian mixture models (HMM-GMM). Our experimental results show that the proposed novel front-end de-noising auto-encoder outperforms the best record we can find for the Aurora 2.0 clean-condition training tasks by an absolute improvement of 1.2% (6.0% vs. 7.2%). In addition, the proposed end-to-end back-end architecture is as good as the traditional HMM-GMM back-end recognizer.",
        "Authors": [
            "Tzu-Hsuan Ting; National Sun Yat-sen University",
            "Chia-Ping Chen; National Sun Yat-sen University"
        ],
        "Paper Title": "COMBINING DE-NOISING AUTO-ENCODER AND RECURRENT NEURAL NETWORKS IN END-TO-END AUTOMATIC SPEECH RECOGNITION FOR NOISE ROBUSTNESS",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "End-to-end automatic speech recognition (ASR) promises to simplify model training and deployment. Most end-to-end ASR systems utilize a bi-directional Long Short-Term Memory (BiLSTM) acoustic model due to its ability to capture acoustic context from the entire utterance. However, BiLSTM models have a high latency and cannot be used in streaming applications. Leveraging knowledge distillation to train a low-latency end-to-end uni-directional LSTM (UniLSTM) model from a BiLSTM model can be an option. However, it makes the strict assumption of shared frame-wise time alignments between the two models. We propose an improved knowledge distillation algorithm that relaxes this assumption and improves the accuracy of the UniLSTM model. We confirmed the advantage of the proposed method on a standard English conversational telephone speech recognition task.",
        "Authors": [
            "Gakuto Kurata; IBM Research",
            "Kartik Audhkhasi; IBM Research"
        ],
        "Paper Title": "IMPROVED KNOWLEDGE DISTILLATION FROM BI-DIRECTIONAL TO UNI-DIRECTIONAL LSTM CTC FOR END-TO-END SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In automatic speech recognition (ASR) what a user says depends on the particular context she is in. Typically, this context is represented as a set of word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E) ASR sys- tem that utilizes such context. Our approach, which we re- fer to as Contextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR components along with embeddings of the context n-grams. During inference, the CLAS system can be presented with context phrases which might contain out-of- vocabulary (OOV) terms not seen during training. We com- pare our proposed system to a more traditional contextualiza- tion approach, which performs shallow-fusion between inde- pendently trained LAS and contextual n-gram models during beam search. Across a number of tasks, we find that the pro- posed CLAS system outperforms the baseline method by as much as 68% relative WER, indicating the advantage of joint optimization over individually trained components.",
        "Authors": [
            "Golan Pundak; Google",
            "Tara N. Sainath; Google",
            "Rohit Prabhavalkar; Google",
            "Anjuli Kannan; Google",
            "Ding Zhao; Google"
        ],
        "Paper Title": "DEEP CONTEXT: END-TO-END CONTEXTUAL SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is re-trained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.",
        "Authors": [
            "Tomoki Hayashi; Nagoya University",
            "Shinji Watanabe; Johns Hopkins University",
            "Yu Zhang; Google",
            "Tomoki Toda; Nagoya University",
            "Takaaki Hori; Mitsubishi Electric Research Laboratories",
            "Ramon Astudillo; INESC-ID-Lisboa",
            "Kazuya Takeda; Nagoya University"
        ],
        "Paper Title": "BACK-TRANSLATION-STYLE DATA AUGMENTATION FOR END-TO-END ASR",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Existing speech recognition systems are typically built at the sentence level, although it is known that dialog context, e.g. higher-level knowledge that spans across sentences or speakers, can help the processing of long conversations. The recent progress in end-to-end speech recognition systems promises to integrate all available information (e.g. acoustic, language resources) into a single model, which is then jointly optimized. It seems natural that such dialog context information should thus also be integrated into the end-to-end models to improve recognition accuracy further. In this work, we present a dialog-context aware speech recognition model, which explicitly uses context information beyond sentence-level information, in an end-to-end fashion. Our dialog-context model captures a history of sentence-level contexts so that the whole system can be trained with dialog-context information in an end-to-end manner. We evaluate our proposed approach on the Switchboard conversational speech corpus and show that our system outperforms a comparable sentence-level end-to-end speech recognition system.",
        "Authors": [
            "Suyoun Kim; Carnegie Mellon University",
            "Florian Metze; Carnegie Mellon University"
        ],
        "Paper Title": "DIALOG-CONTEXT AWARE END-TO-END SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Friday, December 21, 10:00 - 12:00",
        "Session": "ASR III (End-to-End)",
        "Session Time": "Friday, December 21, 10:00 - 12:00",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Current state-of-the-art automatic speech recognition systems are trained to work in specific `domains', defined based on factors like application, sampling rate and codec. When such recognizers are used in conditions that do not match the training domain, performance significantly drops. This work explores the idea of building a single domain-invariant model for varied use-cases by combining large scale training data from multiple application domains. Our final system is trained using 162,000 hours of speech. Additionally, each utterance is artificially distorted during training to simulate effects like background noise, codec distortion, and sampling rates. Our results show that, even at such a scale, a model thus trained works almost as well as those fine-tuned to specific subsets: A single model can be robust to multiple application domains, and variations like codecs and noise. More importantly, such models generalize better to unseen conditions and allow for rapid adaptation -- we show that by using as little as 10 hours of data from a new domain, an adapted domain-invariant model can match performance of a domain-specific model trained from scratch using 70 times as much data. We also highlight some of the limitations of such models and areas that need addressing in future work.",
        "Authors": [
            "Arun Narayanan; Google",
            "Ananya Misra; Google",
            "Khe Chai Sim; Google",
            "Golan Pundak; Google",
            "Anshuman Tripathi; Google",
            "Mohamed Elfeky; Google",
            "Parisa Haghani; Google",
            "Trevor Strohman; Google",
            "Michiel Bacchiani; Google"
        ],
        "Paper Title": "TOWARD DOMAIN-INVARIANT SPEECH RECOGNITION VIA LARGE SCALE TRAINING",
        "Presentation": "Poster",
        "Presentation #": "1",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Code-switching is a commonly occuring phenomenon in many multilingual communities, wherein a speaker switches between languages within a single utterance. Conventional Word Error Rate (WER) is not sufficient for measuring the performance of an Automated Speech Recognition (ASR) system on code-mixed languages due to ambiguities in transcription, misspellings and borrowing of words from two different writing systems. These rendering errors artificially inflate the WER of an ASR system and complicate its evaluation. Furthermore, these errors make it harder to accurately evaluate modeling errors originating from the code-switched language and acoustic models. In this work, we propose the use of a new metric, transliteration-optimized Word Error Rate (toWER) that smoothes out many of these irregularities by mapping all text to one writing system and demonstrate a correlation with the amount of code-switching present in a language. We also present a novel approach to acoustic and language modeling for bilingual code-switched indic languages using the same transliteration approach. We demonstrate the robustness and generality of our proposed approach on state-of-the-art Neural Network based acoustic and language models. We obtain significant gains in ASR performance of up to 10% relative on Google Voice Search and dictation traffic in several Indic languages.",
        "Authors": [
            "Jesse Emond; Google",
            "Bhuvana Ramabhadran; Google",
            "Brian Roark; Google",
            "Pedro Moreno; Google",
            "Min Ma; Google"
        ],
        "Paper Title": "TRANSLITERATION BASED APPROACHES TO IMPROVE CODE-SWITCHED SPEECH RECOGNITION PERFORMANCE",
        "Presentation": "Poster",
        "Presentation #": "2",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Traditional LSTM model and its variants normally work in a frame-by-frame and layer-by-layer fashion, which deals with the temporal modeling and target classification problems at the same time. In this paper, we extend our recently proposed layer trajectory LSTM (ltLSTM) and present a generalized framework, which is equipped with a depth processing block that scans the hidden states of each time-LSTM layer, and uses the summarized layer trajectory information for final senone classification. We explore different modeling units used in the depth processing block to have a good tradeoff between accuracy and runtime cost. Furthermore, we integrate an attention module into this framework to explore wide context information, which is especially beneficial for uni-directional LSTMs. Trained with 30 thousand hours of EN-US Microsoft internal data and cross entropy criterion, the proposed generalized ltLSTM performed significantly better than the standard multi-layer time-LSTM, with up to 12.8% relative word error rate (WER) reduction across different tasks. With attention modeling, the relative WER reduction can be up to 17.9%. We observed similar gain when the models were trained with sequence discriminative training criterion.",
        "Authors": [
            "Jinyu Li; Microsoft",
            "Liang Lu; Microsoft",
            "Changliang Liu; Microsoft",
            "Yifan Gong; Microsoft"
        ],
        "Paper Title": "Exploring layer trajectory LSTM with depth processing units and attention",
        "Presentation": "Poster",
        "Presentation #": "3",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Multi-channel signal processing techniques have played an important role in the far-field automatic speech recognition (ASR) as the separate front-end enhancement part. How- ever, they often meet the mismatch problem. In this paper, we proposed a novel architecture of acoustic model, in which the multi-channel speech without preprocessing was utilized directly. Besides the strategy of knowledge distillation and the generalized cross correlation (GCC) adaptation were em- ployed. We use knowledge distillation to transfer knowledge from a well-trained close-talking model to distant-talking s- cenarios in every frame of the multichannel distant speech. Moreover, the GCC between microphones, which contains the spatial information, is supplied as an auxiliary input to the neural network. We observe good compensation of those two techniques. Evaluated with the AMI and ICSI meeting corpo- ra, the proposed methods achieve relative WER improvement of 7.7% and 7.5% over the model trained directly on the con- catenated multi-channel speech.",
        "Authors": [
            "Wenjie Li; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics",
            "Yu Zhang; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics",
            "Pengyuan Zhang; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics",
            "Fengpei Ge; Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics"
        ],
        "Paper Title": "MULTICHANNEL ASR WITH KNOWLEDGE DISTILLATION AND GENERALIZED CROSS CORRELATION FEATURE",
        "Presentation": "Poster",
        "Presentation #": "4",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Training the distinction of vowel lengths or learning to differentiate between voiced and voiceless plosive sounds in form of minimal pair differentiation is one of the treatments fostering phonological awareness for people with reading and/or writing disabilities. While text-to-speech systems can automatically generate minimal pairs (e.g., bin and pin), the quality of the pronunciation of pseudowords is not always optimal. We present a novel approach for using text-to-speech tools to artificially generate the pronunciation of German pseudowords, which is evaluated in a crowdsourcing task of the discrimination of minimal pairs. While the input for generating audio files for real words is provided as plaintext, the audio files for pseudowords are generated from the SAMPA transcription, a computer-readable phonetic alphabet, of their real-word counterparts. The task of selecting the correct word from a minimal pair of a pseudoword and its lexical counterpart was completed equally successfully when a pseudoword was generated by our method or pronounced by a human (Chi^2(1) = 2.43, p = .119).}",
        "Authors": [
            "Heiko Holz; University of Tübingen",
            "Maria Chinkina; University of Tübingen",
            "Laura Vetter; Ludwig Maximilian University of Munich"
        ],
        "Paper Title": "Optimizing the Quality of Synthetically Generated Pseudowords for the Task of Minimal-Pair Distinction",
        "Presentation": "Poster",
        "Presentation #": "5",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Encoder-decoder models for acoustic-to-word (A2W) automatic speech recognition (ASR) are attractive for their simplicity of architecture and run-time latency while achieving state-of-the-art performances. However, word-based models commonly suffer from the out-of-vocabulary (OOV) word problem. They also cannot leverage text data to improve their language modeling capability. Recently, sequence-to-sequence neural speech synthesis models trainable from corpora have been developed and shown to achieve naturalness comparable to recorded human speech. In this paper, we explore how we can leverage the current speech synthesis technology to tailor the ASR system for a target domain by preparing only a relevant text corpus. From a set of target domain texts, we generate speech features using a sequence-to-sequence speech synthesizer. These artificial speech features together with real speech features from conventional speech corpora are used to train an attention-based A2W model. Experimental results show that the proposed approach improves the word accuracy significantly compared to the baseline trained only with the real speech, although synthetic part of the training data comes only from a single female speaker voice.",
        "Authors": [
            "Masato Mimura; Kyoto University",
            "Sei Ueno; Kyoto University",
            "Hirofumi Inaguma; Kyoto University",
            "Shinsuke Sakai; Kyoto University",
            "Tatsuya Kawahara; Kyoto University"
        ],
        "Paper Title": "LEVERAGING SEQUENCE-TO-SEQUENCE SPEECH SYNTHESIS FOR ENHANCING ACOUSTIC-TO-WORD SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "6",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In Automatic Speech Recognition, it is still challenging to learn useful intermediate representations when using high-level (or abstract) target units such as words. For that reason, when only a few hundreds of hours of training data are available, character or phoneme-based systems tend to outperform word-based systems. In this paper, we show how Hierarchical Multitask Learning can encourage the formation of useful intermediate representations. We achieve this by performing Connectionist Temporal Classification at different levels of the network with targets of different granularity. Our model thus performs predictions in multiple scales for the same input. On the standard 300h Switchboard training setup, our hierarchical multitask architecture demonstrates improvements over single-task architectures with the same number of parameters. Our model obtains 14.0% Word Error Rate on the Switchboard subset of the Eval2000 test set without any decoder or language model, outperforming the current state-of-the-art on non-autoregressive acoustic-to-word models.",
        "Authors": [
            "Ramon Sanabria; Carnegie Mellon University",
            "Florian Metze; Carnegie Mellon University"
        ],
        "Paper Title": "HIERARCHICAL MULTITASK LEARNING WITH CTC",
        "Presentation": "Poster",
        "Presentation #": "7",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Unsupervised spoken term discovery is the task of finding recurrent acoustic patterns in speech without any annotations. Current approaches consists of two steps: (1) discovering similar patterns in speech, and (2) partitioning those pairs of acoustic tokens using graph clustering methods. We propose a new approach for the first step. Previous systems used various approximation algorithms to make the search tractable on large amounts of data. Our approach is based on an optimized k-nearest neighbours (KNN) search coupled with a fixed word embedding algorithm. The results show that the KNN algorithm is robust across languages, consistently outperforms the DTW-based baseline, and is competitive with current state-of-the-art spoken term discovery systems.",
        "Authors": [
            "Alexis Thual; ENS",
            "Corentin Dancette; ENS",
            "Julien Karadayi; ENS",
            "Juan Benjumea; ENS",
            "Emmanuel Dupoux; ENS"
        ],
        "Paper Title": "A K-NEAREST NEIGHBOURS APPROACH TO UNSUPERVISED SPOKEN TERM DISCOVERY",
        "Presentation": "Poster",
        "Presentation #": "8",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "In this work, we apply the recently proposed turbo fusion in conjunction with state-of-the-art convolutional neural networks as acoustic models to the standard phone recognition task on the TIMIT database. The turbo fusion operates on posterior streams stemming from standard filterbank features and from group delay (phase) features. By the iterative exchange of posterior information, the phone error rate is decreased down to 16.91% absolute, which is to our knowledge the best reported result on the TIMIT core test set so far using context-independent acoustic models, outperforming the previous respective benchmark by 4.4% relative.",
        "Authors": [
            "Timo Lohrenz; TU Braunschweig",
            "Wei Li; TU Braunschweig",
            "Tim Fingscheidt; TU Braunschweig"
        ],
        "Paper Title": "A NEW TIMIT BENCHMARK FOR CONTEXT-INDEPENDENT PHONE RECOGNITION USING TURBO FUSION",
        "Presentation": "Poster",
        "Presentation #": "9",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recurrent neural network transducer (RNN-T) has been successfully applied to automatic speech recognition to jointly learn the acoustic and language model components. The RNN-T loss and its gradient with respect to the softmax outputs can be computed efficiently using a forward-backward algorithm. In this paper, we present an efficient implementation of the RNN-T forward-backward and Viterbi algorithms using standard matrix operations. This allows us to easily implement the algorithm in TensorFlow by making use of the existing hardware-accelerated implementations of these operations. This work is based on a similar technique used in our previous work for computing the connectionist temporal classification and lattice-free maximum mutual information losses, where the forward and backward recursions are viewed as a bi-directional RNN whose states represent the forward and backward probabilities. Our benchmark results on graphic processing unit (GPU) and tensor processing unit (TPU) show that our implementation can achieve better throughput performance by increasing the batch size to maximize parallel computation. Furthermore, our implementation is about twice as fast on TPU compared to GPU for batch",
        "Authors": [
            "Tom Bagby; Google",
            "Kanishka Rao; Google",
            "Khe Chai Sim; Google"
        ],
        "Paper Title": "EFFICIENT IMPLEMENTATION OF RECURRENT NEURAL NETWORK TRANSDUCER IN TENSORFLOW",
        "Presentation": "Poster",
        "Presentation #": "10",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recent works in speech recognition rely either on connectionist temporal classification (CTC) or sequence-to-sequence models for character-level recognition. CTC assumes conditional independence of individual characters, whereas attention-based models can provide nonsequential alignments. Therefore, we could use a CTC loss in combination with an attention-based model in order to force monotonic alignments and at the same time get rid of the conditional independence assumption. In this paper, we use the recently proposed hybrid CTC/attention architecture for audio-visual recognition of speech in-the-wild. To the best of our knowledge, this is the first time that such a hybrid architecture architecture is used for audio-visual recognition of speech. We use the LRS2 database and show that the proposed audio-visual model leads to an 1.3% absolute decrease in word error rate over the audio-only model and achieves the new state-of-the-art performance on LRS2 database (7% word error rate). We also observe that the audio-visual model significantly outperforms the audio-based model (up to 32.9% absolute improvement in word error rate) for several different types of noise as the signal-to-noise ratio decreases.",
        "Authors": [
            "Stavros Petridis; Imperial College London",
            "Themos Stafylakis; University of Nottingham",
            "Pingchuan Ma; Imperial College London",
            "Georgios Tzimiropoulos; University of Nottingham",
            "Maja Pantic; Imperial College London"
        ],
        "Paper Title": "AUDIO-VISUAL SPEECH RECOGNITION WITH A HYBRID CTC/ATTENTION ARCHITECTURE",
        "Presentation": "Poster",
        "Presentation #": "11",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multilingual seq2seq model as an initial model, and then perform several transfer learning approaches across 4 other BABEL languages. We also explore different architectures for a multilingual seq2seq model to improve their performance. Further analysis is performed to understand the importance of scheduled sampling approach to bring the model distribution closer to the target data distribution. The paper also discusses about the effect of combining a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the multilingual transfer learning model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant gains, and achieves recognition performance comparable to the models trained with twice more training data.",
        "Authors": [
            "Jaejin Cho; Johns Hopkins University",
            "Murali Karthick Baskar; Brno university of technology",
            "Ruizhi Li; Johns Hopkins University",
            "Matthew Wiesner; Johns Hopkins University",
            "Sri Harish Mallidi; Amazon",
            "Nelson Yalta; Waseda University",
            "Martin Karafiat; Brno university of technology",
            "Shinji Watanabe; Johns Hopkins University",
            "Takaaki Hori; Mitsubishi Electric Research Laboratories"
        ],
        "Paper Title": "Multilingual sequence-to-sequence speech recognition: Architecture, transfer learning, and language modeling",
        "Presentation": "Poster",
        "Presentation #": "12",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper addresses the problem of automatic speech recognition (ASR) of a target speaker in background speech. The novelty of our approach is that we focus on a wakeup keyword, which is usually used for activating ASR systems like smart speakers. The proposed method firstly utilizes a DNN-based mask estimator to separate the mixture signal into the keyword signal uttered by the target speaker and the remaining background speech. Then the separated signals are used for calculating a beamforming filter to enhance the subsequent utterances from the target speaker. Experimental evaluations show that the trained DNN-based mask can selectively separate the keyword and background speech from the mixture signal. The effectiveness of the proposed method is also verified with Japanese ASR experiments, and we confirm that the character error rates are significantly improved by the proposed method for both simulated and real recorded test sets.",
        "Authors": [
            "Yusuke Kida; Yahoo Japan Corporation",
            "Dung Tran; Yahoo Japan Corporation",
            "Motoi Omachi; Yahoo Japan Corporation",
            "Toru Taniguchi; Yahoo Japan Corporation",
            "Yuya Fujita; Yahoo Japan Corporation"
        ],
        "Paper Title": "SPEAKER SELECTIVE BEAMFORMER WITH KEYWORD MASK ESTIMATION",
        "Presentation": "Poster",
        "Presentation #": "13",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "This paper presents, in the context of multi-channel ASR, a method to adapt a mask based, statistically optimal beamforming approach to a speaker of interest. The beamforming vector of the statistically optimal beamformer is computed by utilizing speech and noise masks, which are estimated by a neural network. The proposed adaptation approach is based on the integration of the beamformer, which includes the mask estimation network, and the acoustic model of the ASR system. This allows for the propagation of the training error, from the acoustic modeling cost function, all the way through the beamforming operation and through the mask estimation network. By using the results of a first pass recognition and by keeping all other parameters fixed, the mask estimation network can therefore be fine tuned by retraining. Utterances of a speaker of interest can thus be used in a two pass approach, to optimize the beamforming for the speech characteristics of that specific speaker. It is shown that this approach improves the ASR performance of a state-ofthe- art multi-channel ASR system on the CHiME-4 data. Furthermore the effect of the adaptation on the estimated speech masks is discussed.",
        "Authors": [
            "Tobias Menne; RWTH Aachen University",
            "Ralf Schlüter; RWTH Aachen University",
            "Hermann Ney; RWTH Aachen University"
        ],
        "Paper Title": "SPEAKER ADAPTED BEAMFORMING FOR MULTI-CHANNEL AUTOMATIC SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "14",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "We propose two approaches for speaker adaptation in end-to-end (E2E) automatic speech recognition systems. One is Kullback-Leibler divergence (KLD) regularization and the other is multi-task learning (MTL). Both approaches aim to address the data sparsity especially output target sparsity issue of speaker adaptation in E2E systems. The KLD regularization adapts a model by forcing the output distribution from the adapted model to be close to the unadapted one. The MTL utilizes a jointly trained auxiliary task to improve the performance of the main task. We investigated our approaches on E2E connectionist temporal classification (CTC) models with three different types of output units. Experiments on the Microsoft short message dictation task demonstrated that MTL outperforms KLD regularization. In particular, the MTL adaptation obtained 8.8% and 4.0% relative word error rate reductions (WERRs) for supervised and unsupervised adaptations for the word CTC model, and produced 9.6% and 3.8% relative WERRs for the mix-unit CTC model, respectively.",
        "Authors": [
            "Ke Li; Johns Hopkins University",
            "Jinyu Li; Microsoft AI and Research",
            "Yong Zhao; Microsoft AI and Research",
            "Kshitiz Kumar; Microsoft AI and Research",
            "Yifan Gong; Microsoft AI and Research"
        ],
        "Paper Title": "SPEAKER ADAPTATION FOR END-TO-END CTC MODELS",
        "Presentation": "Poster",
        "Presentation #": "15",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Spectral mapping uses a deep neural network (DNN) to map directly from noisy speech to clean speech. Our previous study found that the performance of spectral mapping improves greatly when using helpful cues from an acoustic model trained on clean speech. The mapper network learns to mimic the input favored by the spectral classifier and cleans the features accordingly. In this study, we explore two new innovations: we replace a DNN-based spectral mapper with a residual network that is more attuned to the goal of predicting clean speech. We also examine how integrating long term context in the mimic criterion (via wide-residual biLSTM networks) affects the performance of spectral mapping compared to DNNs. Our goal is to derive a model that can be used as a preprocessor for any recognition system; the features derived from our model are passed through the standard Kaldi ASR pipeline and achieve a WER of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using only feature adaptation.",
        "Authors": [
            "Peter Plantinga; The Ohio State University",
            "Deblin Bagchi; The Ohio State University",
            "Eric Fosler-Lussier; The Ohio State University"
        ],
        "Paper Title": "AN EXPLORATION OF MIMIC ARCHITECTURES FOR RESIDUAL NETWORK BASED SPECTRAL MAPPING",
        "Presentation": "Poster",
        "Presentation #": "16",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Although advances in close-talk speech recognition have resulted in relatively low error rates, the recognition performance in far-field environments is still limited due to low signal-to-noise ratio, reverberation, and overlapped speech from simultaneous speakers which is especially more difficult. To solve these problems, beamforming and speech separation networks were previously proposed. However, they usually suffer from the leaky speech phenomenon or limited performance due to poor generalization. In this work, we propose a simple yet effective method for multi-channel far-field overlapped speech recognition. In the proposed system, three different features are formed for each target speaker, namely, spectral, spatial, and angle features. Then a neural network is trained using all features with a target of the clean speech of the required speaker. An iterative optimization procedure is proposed in which the mask-based beamforming and mask estimation are performed alternatively. The proposed system were evaluated with real recorded meetings with different levels of overlapping ratios. The results show that the proposed system achieves more than 24\\% relative word error rate (WER) reduction than fixed beamforming with oracle selection. Moreover, as overlap ratio rises from 20\\% to 70+\\%, only 3.8\\% WER increase is observed for the proposed system.",
        "Authors": [
            "Zhuo Chen; Microsoft Cloud & AI",
            "Xiong Xiao; Microsoft Cloud & AI",
            "Takuya Yoshioka; Microsoft Cloud & AI",
            "Jinyu Li; Microsoft Cloud & AI",
            "Hakan Erdogan; Microsoft Cloud & AI",
            "Yifan Gong; Microsoft Cloud & AI"
        ],
        "Paper Title": "Multi-channel multi-speaker overlapped speech recognition with location guided speech extraction network",
        "Presentation": "Poster",
        "Presentation #": "17",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Numerous studies have investigated the effectiveness of neural network quantization on pattern classification tasks. The present study, for the first time, investigated the performance of speech enhancement (a regression task in speech processing) using a novel exponent-only floating-point quantized neural network (EOFP-QNN). The proposed EOFP-QNN consists of two stages: mantissa-quantization and exponent-quantization. In the mantissa-quantization stage, EOFP-QNN learns how to quantize the mantissa bits of the model parameters while preserving the regression accuracy in the least mantissa precision. In the exponent-quantization stage, the exponent part of the parameters is further quantized without any additional performance degradation. We evaluated the proposed EOFP quantization technique on two types of neural networks, namely, bidirectional long short-term memory (BLSTM) and fully convolutional neural network (FCN), on a speech enhancement task. Experimental results showed that the model sizes can be significantly reduced (the model sizes of the quantized BLSTM and FCN models were only 18.75% and 21.89%, respectively, compared to those of the original models) while maintaining a satisfactory speech-enhancement performance.",
        "Authors": [
            "Yi-Te Hsu; Academia Sinica",
            "Yu-Chen Lin; National Taiwan University",
            "Szu-Wei Fu; National Taiwan University",
            "Yu Tsao; Academia Sinica",
            "Tei-Wei Kuo; National Taiwan University"
        ],
        "Paper Title": "A STUDY ON SPEECH ENHANCEMENT USING EXPONENT-ONLY FLOATING POINT QUANTIZED NEURAL NETWORK (EOFP-QNN)",
        "Presentation": "Poster",
        "Presentation #": "18",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Deep neural networks (DNN) have achieved significant success in the field of automatic speech recognition. Previously, we proposed a filterbank-incorporated DNN which takes power spectra as input features. This method has a function of VTLN (Vocal tract length normalization) and fMLLR (feature-space maximum likelihood linear regression). The filterbank layer can be implemented by using a small number of parameters and is optimized under a framework of backpropagation. Therefore, it is advantageous in adaptation under limited available data. In this paper, speaker adaptation is applied to the filterbank-incorporated DNN. By applying speaker adaptation using 15 utterances, the adapted model gave a 7.4% relative improvement in WER over the baseline DNN at a significance level of 0.005 on CSJ task. Adaptation of filterbank layer also showed better performance than the other adaptation methods; singular value decomposition (SVD) based adaptation and learning hidden unit contributions (LHUC).",
        "Authors": [
            "Hiroshi Seki; Toyohashi University of Technology",
            "Kazumasa Yamamoto; Chubu University",
            "Tomoyosi Akiba; Toyohashi University of Technology",
            "Seiichi Nakagawa; Chubu University"
        ],
        "Paper Title": "RAPID SPEAKER ADAPTATION OF NEURAL NETWORK BASED FILTERBANK LAYER FOR AUTOMATIC SPEECH RECOGNITION",
        "Presentation": "Poster",
        "Presentation #": "19",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Far-field automatic speech recognition (ASR) of conversational speech is often considered to be a very challenging task due to poor quality of alignments available for training the DNN acoustic models. A common way to alleviate this problem is to use clean alignments obtained from parallelly recorded close-talk speech. In this work, we advance the parallel data approach by obtaining enhanced low-rank and sparse soft targets from a close-talk ASR system and using them for training more accurate far-field acoustic models. Specifically, we exploit \\textit{eigenposteriors} and \\textit{Compressive Sensing} dictionaries to learn low-dimensional senone subspaces in DNN posterior space, and enhance close-talk DNN posteriors to obtain high quality soft targets. Enhanced soft targets encode the structural and temporal inter-relationships among senone classes which are easily accessible in the DNN posterior space of close-talk speech but not in its noisy far-field counterpart. We exploit enhanced soft targets to improve the mapping of far-field acoustics to close-talk senone classes. Experiments are performed on AMI corpus where our approach improves DNN acoustic modeling by 4.4\\% absolute reduction in WER as compared to a system which doesn't use parallel data. Finally, the approach is also validated on state-of-the-art recurrent and time delay neural network architectures.",
        "Authors": [
            "Pranay Dighe; Idiap Research Institute, EPFL",
            "Afsaneh Asaei; Idiap Research Institute",
            "Herve Bourlard; Idiap Research Institute, EPFL"
        ],
        "Paper Title": "FAR-FIELD ASR USING LOW-RANK AND SPARSE SOFT TARGETS FROM PARALLEL DATA",
        "Presentation": "Poster",
        "Presentation #": "20",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Speech recognition and synthesis:"
    },
    {
        "Abstract": "Recently, visual-only and audio-visual speech recognition have made significant progress thanks to deep-learning based, trainable visual front-ends (VFEs), with most research focusing on frontal or near-frontal face videos. In this paper, we seek to expand the applicability of VFEs targeted on frontal face views to non-frontal ones, without making assumptions on the VFE type, and allowing systems trained on frontal-view data to be applied on mismatched, non-frontal videos. For this purpose, we adapt the “pix2pix” model, recently proposed for image translation tasks, to transform non-frontal speaker mouth regions to frontal, employing a convolutional neural network architecture, which we call “view2view”. We develop our approach on the OuluVS2 multiview lipreading dataset, allowing training of four such networks that map views at predefined non-frontal angles (up to profile) to frontal ones, which we subsequently feed to a frontal-view VFE. We compare the “view2view” network against a baseline that performs linear cross-view regression at the VFE space. Results on visual-only, as well as audio-visual automatic speech recognition over multiple acoustic noise conditions, demonstrate that the “view2view” significantly outperforms the baseline, narrowing the performance gap from an ideal, matched scenario of view-specific systems.",
        "Authors": [
            "Alexandros Koumparoulis; National Technical University of Athens",
            "Gerasimos Potamianos; University of Thessaly"
        ],
        "Paper Title": "DEEP VIEW2VIEW MAPPING FOR VIEW-INVARIANT LIPREADING",
        "Presentation": "Poster",
        "Presentation #": "21",
        "Presentation Time": "Friday, December 21, 13:30 - 15:30",
        "Session": "ASR IV",
        "Session Time": "Friday, December 21, 13:30 - 15:30",
        "Topic": "Multimodal processing:"
    }
]